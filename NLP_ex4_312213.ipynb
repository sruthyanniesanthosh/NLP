{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1c5cv2w2ImO"
   },
   "source": [
    "# Practical exercise 4 - experiments with Wordnet\n",
    "\n",
    "WordNet is a large digital lexicon made by hand. The kernel of WordNet are the so called synsets that can be understood as meanings. Each word belongs to one or more synsets and each synset is made up of one or more words. Semantic relations like hypernymy and hyponymy exist between synsets, not between words! Consequently, there is no such thing like synonymy in Wordnet. If two words are synonymous the will share one or several synsets.<br>\n",
    "It is possible to access Wordnet is via the web interface: http://wordnetweb.princeton.edu/perl/webwn. There we can see e.g. the synsets\n",
    "of a word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlQOh0mujYeT"
   },
   "source": [
    "# 1. WordNet in Python\n",
    "\n",
    "The NLTK package offers some easy methods to access WordNet. Before you use WordNet you have to run once the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHfttgQnjpUi",
    "outputId": "02950680-5e18-4af0-e751-1dbbe71e99ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sruthysanthosh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sruthysanthosh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVsKddF9kVRJ"
   },
   "source": [
    "How to access synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luBhaPNJj9NE",
    "outputId": "93b980a3-749c-4f11-97a5-0fd492b1a03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rock.n.01')\n",
      "Synset('rock.n.02')\n",
      "Synset('rock.n.03')\n",
      "Synset('rock.n.04')\n",
      "Synset('rock_candy.n.01')\n",
      "Synset('rock_'n'_roll.n.01')\n",
      "Synset('rock.n.07')\n",
      "Synset('rock.v.01')\n",
      "Synset('rock.v.02')\n",
      "\n",
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "[Lemma('dog.n.01.dog'), Lemma('dog.n.01.domestic_dog'), Lemma('dog.n.01.Canis_familiaris')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# get synsets of a word\n",
    "synsets = wn.synsets(\"rock\")\n",
    "for s in synsets:\n",
    "    print(s)\n",
    "print()\n",
    "\n",
    "# use synset identifier directly\n",
    "dog = wn.synset(\"dog.n.01\")\n",
    "print(dog.hypernyms())\n",
    "print(dog.hyponyms())\n",
    "print(dog.lemmas())  # ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZPvJKVys2UW"
   },
   "source": [
    "An easy way to compute the similarity between two synsets is to measure the length of the path between the synsets in the WordNet hierarchy made up by the hypernym relations. The method path_simiarity returns 1/p where p is the length of the path between two synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-SrM7VLrXnp",
    "outputId": "078aa484-d188-4efb-a799-12a232ca7749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between ape and monkey:  0.3333333333333333\n",
      "Similarity between ape and zoo:  0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "ape = wn.synset(\"ape.n.01\")\n",
    "monkey = wn.synset(\"monkey.n.01\")\n",
    "zoo = wn.synset(\"zoo.n.01\")\n",
    "\n",
    "print( \"Similarity between ape and monkey: \", ape.path_similarity(monkey))\n",
    "print( \"Similarity between ape and zoo: \", ape.path_similarity(zoo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6PrfM-RtC8i"
   },
   "source": [
    "Wordnet is not completely connected. The path similarity method therefore assumes a fake root node that connect all parts. The path similarity has the problem that words are less similar if they are part of the hierarchy that is worked out in more detail. In general we would assume that the \u001bfirst divisions at the top of the hierarchy imply large semantic differences, while a division at a very deep position in the hierarchy makes only small semantic distinctions. Therefore some alternative measures have been defi\u001bned, e.g. the Wu-Palmer similarity and the Leacock-Chodorow similarity (feel free to read up on those measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iIAT5CasVTg",
    "outputId": "0b820964-3883-4eb5-ef8c-dea400e5c369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wu竏単almer similarity between ape and monkey:  0.9230769230769231\n",
      "Wu竏単almer similarity between ape and zoo:  0.4\n",
      "Leacock Chodorow similarity between ape and monkey:  2.538973871058276\n",
      "Leacock Chodorow similarity between ape and zoo:  1.072636802264849\n"
     ]
    }
   ],
   "source": [
    "print(\"Wu竏単almer similarity between ape and monkey: \", ape.wup_similarity(monkey))\n",
    "print(\"Wu竏単almer similarity between ape and zoo: \", ape.wup_similarity(zoo))\n",
    "\n",
    "print(\"Leacock Chodorow similarity between ape and monkey: \", ape.lch_similarity(monkey))\n",
    "print(\"Leacock Chodorow similarity between ape and zoo: \", ape.lch_similarity(zoo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz6udeQWtsXT"
   },
   "source": [
    "Both measures give higher weight to distances between nodes that are closer to the root. However, the distance to the root is also a design decision and a number of measures try to include other information sources as well. E.g. the similarity measures of Resnik and Lin include the frequency of words in a corpus as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tTTvM1w48FN"
   },
   "source": [
    "# 2. Exercise:\n",
    "\n",
    "0. Read in the email dataset (see exercise 3). You may copy some of the code from that notebook.\n",
    "\n",
    "1. Let us investigate the coverage of this data in Wordnet:\n",
    "    - Count the unique words (types) in the data and store them in a list.\n",
    "    - How many of those items have synsets in Wordnet? (calculate a percentage value)\n",
    "    - What is the average number of synsets per type?\n",
    "\n",
    "2. Not all words have lexical meaning. We can filter certain word classes. Apply POS-tagging (for example https://www.nltk.org/book/ch05.html) to extract only nouns (just NN - not proper nouns NNP). Check the coverage in Wordnet for these nouns. How many have synsets in Wordnet? (calculate a percentage value)\n",
    "\n",
    "3. Experiments with the similarity of words:\n",
    "    - Choose 10 out of the 50 most frequent nouns from the data set (they all should have at least one synset in Wordnet).\n",
    "    - Now compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words (you may use the first synset for each word for this calculation when words have multiple synsets). You might want to display the resulting numbers in a table. Which words are most similar to each other?\n",
    "    - Check for all sentences which contain the word 'Obama': How often does each of the 10 words you selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Reading the email dataset . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "gQIZYzIy7AHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of email bodies: 6741\n",
      "Number of sentences- 32290\n",
      "Sample sentence- ['Unfortunately', ',', 'the', 'European', 'Intelligence', 'services', 'have', 'been', 'unable', 'to', 'confirm', 'or', 'discredit', 'these', 'reports', '.']\n"
     ]
    }
   ],
   "source": [
    "# extract the zip file\n",
    "import zipfile\n",
    "from somajo import SoMaJo\n",
    "import numpy as np\n",
    "\n",
    "with zipfile.ZipFile(\"emails-body.txt.zip\", 'r') as zip_f:\n",
    "    zip_f.extractall('.')\n",
    "texts = open('emails-body.txt').read().split('<cmail>\\n')\n",
    "print(f'number of email bodies: {len(texts)}')\n",
    "\n",
    "#Tokenizing data\n",
    "somajo_tokenizer = SoMaJo(language=\"en_PTB\",\n",
    "                          split_camel_case=True)\n",
    "\n",
    "data_tok = []\n",
    "for sentence in somajo_tokenizer.tokenize_text(texts):\n",
    "    data_tok.append([token.text for token in sentence])\n",
    "print(\"Number of sentences-\",len(data_tok))\n",
    "\n",
    "print(\"Sample sentence-\",data_tok[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the unique words (types) in the data and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of types (unique words): 37340\n",
      "Total number of tokens: 708310\n",
      "the most frequent words:\n",
      "[',', 'the', '.', 'to', 'and', 'of', 'a', 'in', '\"', 'that', \"'s\", 'is', 'for', '-', 'I', 'on', 'with', ':', 'you', 'it']\n"
     ]
    }
   ],
   "source": [
    "# count words and their frequencies\n",
    "from collections import Counter\n",
    "\n",
    "sentences = data_tok\n",
    "\n",
    "words = Counter(word for sentence in sentences for word in sentence)\n",
    "# Note: \"words\" now contains a mapping of words to their frequencies.\n",
    "# total number of types in the corpus\n",
    "print(f'Total number of types (unique words): {len(words)}')\n",
    "\n",
    "# total number of tokens in the corpus\n",
    "print(f'Total number of tokens: {sum(words.values())}')\n",
    "\n",
    "\n",
    "sorted_words = sorted(words, key=lambda word: words[word], reverse=True)\n",
    "\n",
    "print('the most frequent words:')\n",
    "print(sorted_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many of those items have synsets in Wordnet? (calculate a percentage value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "#For each unique word\n",
    "for word in words:\n",
    "    #checking if it has synset\n",
    "    if(wn.synsets(word)):\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words having synsets- 66.24799143010178\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of words having synsets-\", (count/len(words)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of synsets per type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = []\n",
    "for word in words:\n",
    "    #for each unique word having synset\n",
    "    if(wn.synsets(word)):\n",
    "        #appending the number of synsets to an array\n",
    "        flag.append(len(wn.synsets(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of synsets per type- 4.67\n"
     ]
    }
   ],
   "source": [
    "#Finding average of the array\n",
    "avg = np.mean(flag)\n",
    "print(\"Average number of synsets per type-\", round(avg,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply POS-tagging  to extract only nouns (just NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hrod17', 'NN'), ('meat', 'NN'), ('Sent', 'NN'), ('Subject', 'NN'), ('htte', 'NN'), ('maxbiumenthal', 'NN'), ('com12012', 'NN'), ('meet', 'NN'), ('extremist', 'NN'), ('musiim', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "#Applying pos-tagging to all tokens\n",
    "for i in range(len(sentences)):\n",
    "    k+=(nltk.pos_tag(sentences[i]))\n",
    "\n",
    "noun = []\n",
    "#Extracting tokens having NN tag and storing it in another array\n",
    "for i in range(len(k)):\n",
    "    if(k[i][1]== 'NN'):\n",
    "        noun.append(k[i])\n",
    "print(noun[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nouns- 89073\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nouns-\",len(noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many have synsets in Wordnet? (calculate a percentage value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "noun_synset = []\n",
    "#For each noun, checking if it has synset\n",
    "for i in range(len(noun)):\n",
    "    if(wn.synsets(noun[i][0])):\n",
    "        count+=1\n",
    "        noun_synset.append(noun[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of nouns having synsets-  88.346\n"
     ]
    }
   ],
   "source": [
    "percent = count/len(noun)*100\n",
    "print(\"Percentage of nouns having synsets- \",round(percent,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'memo',\n",
       " 'syria',\n",
       " 'print',\n",
       " 'print',\n",
       " 'meat',\n",
       " 'Sent',\n",
       " 'Subject',\n",
       " 'meet',\n",
       " 'extremist',\n",
       " 'PRODUCED',\n",
       " 'H',\n",
       " 'memo',\n",
       " 'print',\n",
       " 'direct',\n",
       " 'Sent',\n",
       " 'direct',\n",
       " 'Sent',\n",
       " 'piece',\n",
       " 'page']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_synset[:20] #Nouns having synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with similarity\n",
    "\n",
    "### Choose 10 out of the 50 most frequent nouns from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding frequency of all the nouns having synsets\n",
    "freq = Counter(noun_synset)\n",
    "freq[\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten most frequent nouns are -  ['pm', 'time', 'government', 'today', 'policy', 'way', 'president', 'world', 'year', 'security']\n"
     ]
    }
   ],
   "source": [
    "sorted_nouns = sorted(freq, key=lambda noun: freq[noun], reverse=True)\n",
    "print(\"Ten most frequent nouns are - \",sorted_nouns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = sorted_nouns[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute for each of the 10 words the Wu-Palmer or Leacock-Chodorow similarity to each of the other 9 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing WUP similarities btw all the 10 nouns and storing it in a matrix\n",
    "#Using two for loops to get all combinations\n",
    "\n",
    "sim= np.zeros((10,10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        #print(i,j)\n",
    "        n1 = wn.synsets(top_10[j])[0]\n",
    "        n2 = wn.synsets(top_10[i])[0]\n",
    "        sim[i][j] = n1.wup_similarity(n2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity of words in a tabular format-\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm</th>\n",
       "      <th>time</th>\n",
       "      <th>government</th>\n",
       "      <th>today</th>\n",
       "      <th>policy</th>\n",
       "      <th>way</th>\n",
       "      <th>president</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>security</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.470588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pm      time  government     today    policy       way  \\\n",
       "pm          1.000000  0.470588    0.235294  0.250000  0.315789  0.266667   \n",
       "time        0.470588  1.000000    0.285714  0.307692  0.375000  0.333333   \n",
       "government  0.235294  0.285714    1.000000  0.307692  0.250000  0.333333   \n",
       "today       0.250000  0.307692    0.307692  1.000000  0.266667  0.545455   \n",
       "policy      0.315789  0.375000    0.250000  0.266667  1.000000  0.285714   \n",
       "way         0.266667  0.333333    0.333333  0.545455  0.285714  1.000000   \n",
       "president   0.100000  0.117647    0.117647  0.125000  0.105263  0.133333   \n",
       "world       0.125000  0.153846    0.153846  0.166667  0.133333  0.181818   \n",
       "year        0.250000  0.307692    0.307692  0.333333  0.266667  0.363636   \n",
       "security    0.235294  0.285714    0.285714  0.461538  0.250000  0.500000   \n",
       "\n",
       "            president     world      year  security  \n",
       "pm           0.100000  0.125000  0.250000  0.235294  \n",
       "time         0.117647  0.153846  0.307692  0.285714  \n",
       "government   0.117647  0.153846  0.307692  0.285714  \n",
       "today        0.125000  0.166667  0.333333  0.461538  \n",
       "policy       0.105263  0.133333  0.266667  0.250000  \n",
       "way          0.133333  0.181818  0.363636  0.500000  \n",
       "president    1.000000  0.421053  0.125000  0.117647  \n",
       "world        0.421053  1.000000  0.166667  0.153846  \n",
       "year         0.125000  0.166667  1.000000  0.307692  \n",
       "security     0.117647  0.153846  0.307692  1.000000  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Displaying the similarity matrix in table form using pandas\n",
    "names = [_ for _ in top_10]\n",
    "df = pd.DataFrame(sim, index=names, columns=names)\n",
    "print(\"The similarity of words in a tabular format-\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum similarity is btw today and way with value 0.545\n"
     ]
    }
   ],
   "source": [
    "# Words which are most similar to each other\n",
    "max_row = 0\n",
    "max_col = 0\n",
    "max_val = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        #finding value in matrix which is the maximum value\n",
    "        if((sim[i][j] > max_val) and (i is not j)):\n",
    "            max_val = sim[i][j]\n",
    "            max_row = i\n",
    "            max_col = j\n",
    "\n",
    "print(\"Maximum similarity is btw {} and {} with value {}\".format(names[max_row],names[max_col], round(max_val,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for all sentences which contain the word 'Obama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentences containing Obama- [['By', 'Anne', '-', 'Marie', 'Slaughter', 'PRESIDENT', 'Obama', 'says', 'the', 'noose', 'is', 'tightening', 'around', 'Col.', 'Muammar', 'al', '-', 'Qaddafi', '.'], ['Americans', 'in', 'turn', 'will', 'read', 'the', 'words', 'of', 'Mr.', 'Obama', \"'s\", 'June', '2009', 'speech', 'in', 'Cairo', '.', ',', 'with', 'its', 'lofty', 'promises', 'to', 'stand', 'for', 'universal', 'human', 'rights', ',', 'and', 'cringe', '.']]\n"
     ]
    }
   ],
   "source": [
    "Obama_sent = []\n",
    "#Finding sentences having the word 'Obama' in them\n",
    "for i in range(len(sentences)):\n",
    "    if('Obama' in sentences[i]):\n",
    "        Obama_sent.append(sentences[i])\n",
    "        \n",
    "print(\"Sample sentences containing Obama-\", Obama_sent[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences having the word 'Obama'-  1156\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences having the word 'Obama'- \",len(Obama_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How often does each of the 10 words you selected occur in these sentences? Have words with similar meaning also similar co-occurrence counts with 'Obama'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_each = np.zeros(10)\n",
    "#Iterating through each sentence and each word in top 10 list\n",
    "for i in range(len(Obama_sent)):\n",
    "    for k in range(10):\n",
    "        #if word is in the Obama sentence, then updating the corresponding counter\n",
    "        if(top_10[k] in Obama_sent[i]):\n",
    "            count_each[k]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pm occurs 8 times\n",
      "\n",
      " time occurs 36 times\n",
      "\n",
      " government occurs 25 times\n",
      "\n",
      " today occurs 24 times\n",
      "\n",
      " policy occurs 73 times\n",
      "\n",
      " way occurs 19 times\n",
      "\n",
      " president occurs 76 times\n",
      "\n",
      " world occurs 23 times\n",
      "\n",
      " year occurs 38 times\n",
      "\n",
      " security occurs 39 times\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"\\n {} occurs {} times\".format(top_10[i],int(count_each[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word 'President' occurs the most in sentences having the word 'Obama'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_matrix = np.zeros((10,10))\n",
    "\n",
    "#for each sentence, for each word pair\n",
    "for i in range(len(Obama_sent)):\n",
    "    for j in range(10):\n",
    "        for k in range(10):\n",
    "            #checking if the word pair occurs in sentence\n",
    "            if ((top_10[k]in Obama_sent[i]) and (top_10[j]in Obama_sent[i]) and (names[k]!= names[j])):\n",
    "                co_matrix[j][k] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 0., 0., 0., 0., 2.],\n",
       "       [1., 0., 0., 1., 1., 0., 4., 4., 1., 3.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 2., 0., 0., 0., 1., 2.],\n",
       "       [1., 1., 0., 2., 0., 0., 3., 3., 2., 2.],\n",
       "       [0., 0., 0., 0., 0., 0., 2., 0., 0., 0.],\n",
       "       [0., 4., 1., 0., 3., 2., 0., 1., 4., 0.],\n",
       "       [0., 4., 1., 0., 3., 0., 1., 0., 1., 2.],\n",
       "       [0., 1., 0., 1., 2., 0., 4., 1., 0., 0.],\n",
       "       [2., 3., 0., 2., 2., 0., 0., 2., 0., 0.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The co occurent count of words in a tabular format-\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm</th>\n",
       "      <th>time</th>\n",
       "      <th>government</th>\n",
       "      <th>today</th>\n",
       "      <th>policy</th>\n",
       "      <th>way</th>\n",
       "      <th>president</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>security</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pm</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>policy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>president</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pm  time  government  today  policy  way  president  world  year  \\\n",
       "pm          0.0   1.0         0.0    0.0     1.0  0.0        0.0    0.0   0.0   \n",
       "time        1.0   0.0         0.0    1.0     1.0  0.0        4.0    4.0   1.0   \n",
       "government  0.0   0.0         0.0    0.0     0.0  0.0        1.0    1.0   0.0   \n",
       "today       0.0   1.0         0.0    0.0     2.0  0.0        0.0    0.0   1.0   \n",
       "policy      1.0   1.0         0.0    2.0     0.0  0.0        3.0    3.0   2.0   \n",
       "way         0.0   0.0         0.0    0.0     0.0  0.0        2.0    0.0   0.0   \n",
       "president   0.0   4.0         1.0    0.0     3.0  2.0        0.0    1.0   4.0   \n",
       "world       0.0   4.0         1.0    0.0     3.0  0.0        1.0    0.0   1.0   \n",
       "year        0.0   1.0         0.0    1.0     2.0  0.0        4.0    1.0   0.0   \n",
       "security    2.0   3.0         0.0    2.0     2.0  0.0        0.0    2.0   0.0   \n",
       "\n",
       "            security  \n",
       "pm               2.0  \n",
       "time             3.0  \n",
       "government       0.0  \n",
       "today            2.0  \n",
       "policy           2.0  \n",
       "way              0.0  \n",
       "president        0.0  \n",
       "world            2.0  \n",
       "year             0.0  \n",
       "security         0.0  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the co occurrence matrix in table form using pandas\n",
    "names = [_ for _ in top_10]\n",
    "df = pd.DataFrame(co_matrix, index=names, columns=names)\n",
    "print(\"The co occurent count of words in a tabular format-\\n\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words which occured the most together are 'time' and 'president' which occured 4 times\n",
      "The words which occured the most together are 'time' and 'world' which occured 4 times\n",
      "The words which occured the most together are 'president' and 'time' which occured 4 times\n",
      "The words which occured the most together are 'president' and 'year' which occured 4 times\n",
      "The words which occured the most together are 'world' and 'time' which occured 4 times\n",
      "The words which occured the most together are 'year' and 'president' which occured 4 times\n"
     ]
    }
   ],
   "source": [
    "# Words which occur together the most \n",
    "max_row = 0\n",
    "max_col = 0\n",
    "max_val = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        #finding value in matrix which is the maximum value\n",
    "        if((co_matrix[i][j] > max_val) and (i is not j)):\n",
    "                \n",
    "                max_val = co_matrix[i][j]\n",
    "                max_row = i\n",
    "                max_col = j\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if(co_matrix[i][j] == max_val):\n",
    "            \n",
    "            print(\"The words which occured the most together are '{}' and '{}' which occured {} times\"\n",
    "      .format(names[i],names[j], int(max_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that time-president, time-world and year-president occurs the most together ( 4 times ). But they do not have high similarity meaning values.\n",
    "\n",
    "Hence words with similar meaning dont have similar co occurence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-2022_exercise4_Wordnet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
