{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184fc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of all required libraries\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import tarfile\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from scipy.spatial import distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea8f7e",
   "metadata": {},
   "source": [
    "## Downloading the datasets - SemCor, Senseval-2 and Senseval-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa0c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same methods are followed to download all 3 datasets\n",
    "\n",
    "#file_url = 'http://web.eecs.umich.edu/~mihalcea/downloads/senseval.semcor/senseval2.semcor.tar.gz'\n",
    "#fname = 'senseval2.semcor.tar.gz'\n",
    "\n",
    "#file_url = 'http://web.eecs.umich.edu/~mihalcea/downloads/senseval.semcor/senseval3.semcor.tar.gz'\n",
    "#fname = 'senseval3.semcor.tar.gz'\n",
    "\n",
    "file_url = 'http://web.eecs.umich.edu/~mihalcea/downloads/semcor/semcor1.7.tar.gz'\n",
    "fname = 'semcor1.7.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d56e22",
   "metadata": {},
   "source": [
    "#Extracting the files in the datasets , needed only once for each dataset\n",
    "\n",
    "local_filename, _ = urlretrieve(file_url, fname)\n",
    "tar = tarfile.open(fname, \"r:gz\")\n",
    "tar.extractall()\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef6573",
   "metadata": {},
   "source": [
    "### Mapping the datasets, to get sentences from the dataset\n",
    "\n",
    "### For each word in sentence, we get its lemma, pos tag and wnsn (ground truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51e5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing dictionaries to store sentences in each dataset\n",
    "data_dict_semcor = {}\n",
    "data_dict_senseval2 = {}\n",
    "data_dict_senseval3 = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7c7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary for mapping postags in dataset to postags in AutoExtend\n",
    "\n",
    "dict_postags = {\"NN\":\"n\", \"RB\":\"r\", \"JJ\":\"a\", \"VB\":\"v\", \"NNP\":\"n\",\"NNPS\":\"n\",\"NP\":\"n\",\"NPS\":\"n\", \"NNS\":\"n\",\"JJR\":\"a\",\"JJS\":\"a\", \"RBR\":\"r\", \"RBS\":\"r\", \"VBD\":\"v\", \"VBG\":\"v\",\"VBN\":\"v\",\"VBZ\":\"v\",\"VBP\":\"v\",\"WRB\":\"r\",\"MD\":\"v\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49bd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Semcor dataset\n",
    "\n",
    "rootdir = 'semcor1.7'\n",
    "arr_sem_cor=[]\n",
    "count = 0\n",
    "\n",
    "#Retrieving the file paths\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if 'br' in file:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_sem_cor.append(var1)\n",
    "\n",
    "#Reading the sentences in each file\n",
    "for url in arr_sem_cor:\n",
    "\n",
    "    \n",
    "    # Make a GET request to fetch the raw HTML content\n",
    "    with open(url) as fp:\n",
    "        #Reading using Beautiful Soup\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "   \n",
    "    #Finding all s tags -> denotes start of a sentence\n",
    "    for i in soup.findAll('s'):\n",
    "        \n",
    "        sentence = \"\"\n",
    "        value = []\n",
    "        sub_dict = {}\n",
    "        \n",
    "        #Required words are only in wf tags having cmd attribute as \"done\"\n",
    "        for j in i.findAll('wf', {\"cmd\":\"done\"}):\n",
    "            \n",
    "            #Checking whether the word has a lemma attribute\n",
    "            if (type(j.get(\"lemma\")) == str):\n",
    "                flag = True\n",
    "                \n",
    "                #Checking whether the word has id\n",
    "                if (not(j.get(\"ot\"))):\n",
    "                    \n",
    "                    #Forming sentence by joining the words\n",
    "                    sentence = sentence + j.text\n",
    "                    sentence += \" \"\n",
    "                    \n",
    "                    #If the word has a postag which can be mapped\n",
    "                    if j.get(\"pos\") in dict_postags:\n",
    "\n",
    "                        #We form the phrase using the attribute values in the word tag (lemma-postag-wnsn )\n",
    "                        phrase= j.get(\"lemma\")+\",\"+dict_postags[j.get(\"pos\")]+\",\"+j.get(\"wnsn\")\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    #For each word in a sentence, store the phrase as its value in a dictionary\n",
    "                    sub_dict[j.text] = phrase\n",
    "            else:\n",
    "                flag = False\n",
    "        \n",
    "        \n",
    "        if flag:\n",
    "            \n",
    "            #store each sentence in the dataset as index: [sentence, sub_dict] form\n",
    "            #sub dictionary contains each word in the sentence and its phrase\n",
    "            value.append(sentence)\n",
    "            value.append(sub_dict)\n",
    "            key = count\n",
    "            data_dict_semcor[key]=value \n",
    "            count = count+1\n",
    "\n",
    "#key - index, value - array with first value as sentence and second value the sub dictionary\n",
    "#phrase is needed for finding the lexeme embedding and for checking the ground truth (wnsn value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e662376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Semcor dataset i.e, number of sentences - 34374\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Semcor dataset i.e, number of sentences -\",len(data_dict_semcor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5dae335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking 5000 random sentences from semcor\n",
    "\n",
    "index_list = []\n",
    "for i in range(0,5000):\n",
    "    x = random.randint(1,34374)\n",
    "    index_list.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading senseval2 dataset\n",
    "\n",
    "#The reading of the dataset and storing in dictionary is in \n",
    "#same format as the semcor dataset \n",
    "\n",
    "#Setting the required root\n",
    "rootdir = 'senseval2.semcor'\n",
    "\n",
    "arr_senseval2_file=[]\n",
    "\n",
    "#Finding the file paths required\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    \n",
    "    if subdir==\"senseval2.semcor/wordnet1.7.1\":\n",
    "        for file in files:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_senseval2_file.append(var1)\n",
    "count=0\n",
    "\n",
    "#for each file\n",
    "for url in arr_senseval2_file:\n",
    "\n",
    "    \n",
    "    # Make a GET request to fetch the raw HTML content\n",
    "    html_content = open(url).read()\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "   \n",
    "    #Form the sentences and words with its corresponding phrases\n",
    "    #Steps followed are same as done for Semcor\n",
    "    \n",
    "    for i in soup.findAll('s'):\n",
    "        \n",
    "        sentence = \" \"\n",
    "        value = []\n",
    "        sub_dict = {}\n",
    "        for j in i.findAll('wf', {\"cmd\":\"done\"}):\n",
    "            if (type(j.get(\"lemma\")) == str):\n",
    "                flag = True\n",
    "                phrase = \" \"\n",
    "                if (not(j.get(\"ot\"))):\n",
    "                    \n",
    "                    sentence = sentence + j.text\n",
    "                    sentence += \" \"\n",
    "                    if j.get(\"pos\") in dict_postags:\n",
    "\n",
    "                        phrase= j.get(\"lemma\")+\",\"+dict_postags[j.get(\"pos\")]+\",\"+j.get(\"wnsn\")\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "                    temp = j.text\n",
    "                    sub_dict[j.text] = phrase\n",
    "            else:\n",
    "                flag = False\n",
    "        \n",
    "        if flag:\n",
    "            value.append(sentence)\n",
    "            value.append(sub_dict)\n",
    "            key = count\n",
    "            data_dict_senseval2[key]=value \n",
    "            count = count+1\n",
    "#key - index, value - array with first value as sentence and second value the sub dictionary\n",
    "#phrase - (lemma-postag-wnsn) is stored for each word in sentence as the sub dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dedf91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in senseval2-  238\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in senseval2- \",len(data_dict_senseval2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61629f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading senseval3 dataset\n",
    "\n",
    "#The steps followed for reading senseval3 dataset is similar to the ones \n",
    "#followed for the other two datasets. Hence I have not added descriptive comments here.\n",
    "#please check the above cells for detailed explanation\n",
    "\n",
    "rootdir = 'senseval3.semcor'\n",
    "\n",
    "#finding file paths\n",
    "arr_senseval3_file=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if subdir==\"senseval3.semcor/wordnet1.7.1\":\n",
    "        for file in files:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_senseval3_file.append(var1)\n",
    "count=0\n",
    "\n",
    "#Reading sentences in each file\n",
    "for url in arr_senseval3_file:\n",
    "\n",
    "    \n",
    "    # Make a GET request to fetch the raw HTML content\n",
    "    html_content = open(url).read()\n",
    "\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "   \n",
    "    # s-tag denotes the start of a sentence\n",
    "    for i in soup.findAll('s'):\n",
    "        \n",
    "        sentence = \" \"\n",
    "        value = []\n",
    "        sub_dict = {}\n",
    "        for j in i.findAll('wf', {\"cmd\":\"done\"}):\n",
    "            if (type(j.get(\"lemma\")) == str):\n",
    "                flag = True\n",
    "                phrase = \" \"\n",
    "                if (not(j.get(\"ot\"))):\n",
    "                   \n",
    "                    sentence = sentence + j.text\n",
    "                    sentence += \" \"\n",
    "\n",
    "                    if j.get(\"pos\") in dict_postags:\n",
    "\n",
    "                        #phrase-(lemma-postag-wnsn) -> required for lexeme embeddings and ground truth\n",
    "                        phrase= j.get(\"lemma\")+\",\"+dict_postags[j.get(\"pos\")]+\",\"+j.get(\"wnsn\")\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    sub_dict[j.text] = phrase\n",
    "            else:\n",
    "                flag = False\n",
    "        \n",
    "        if flag:\n",
    "            value.append(sentence)\n",
    "            value.append(sub_dict)\n",
    "            key = count\n",
    "            data_dict_senseval3[key]=value \n",
    "            count = count+1\n",
    "    #key - index, value - array with first value as sentence and second value the sub dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e55224fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in senseval3 - 300\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in senseval3 -\",len(data_dict_senseval3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d790b2",
   "metadata": {},
   "source": [
    "## Read the Lexeme embeddings from AutoExtend\n",
    "\n",
    "### Pretrained embeddings are downloaded from the AutoExtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b66810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get lexeme embeddings, we need to format the word and the corresponding synset taken in a particular way\n",
    "#format :  lemma-wn-2.1-offset-postag\n",
    "\n",
    "#We get the format of the word from the dataset  \n",
    "#we have to do mapping and form the format for the particular synset using offset function\n",
    "\n",
    "#We know the postag and lemma of the word from the dictionary containing the datasets\n",
    "\n",
    "#Now from lexems.txt, we need to extract the 300 size vector embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff60e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the lines in the lexemes file.\n",
    "#This file contains the sense embeddings for each lemma, postag and synset\n",
    "\n",
    "with open('embeddings 2/lexemes.txt') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "\n",
    "#We store the tags and embeddings from lexeme.txt to a dictionary dict\n",
    "lines= lines[1:]\n",
    "dict = {}\n",
    "for line in lines:\n",
    "    #We split and get the embedding part\n",
    "    key,val = line.split(maxsplit=1)\n",
    "    numbers_str = val\n",
    "    numbers_list = []\n",
    "   \n",
    "    #Store as vector\n",
    "    for num_str in numbers_str.split():\n",
    "        num_int = float(num_str)\n",
    "        numbers_list.append(num_int)\n",
    "    \n",
    "    \n",
    "    #Sense embedding is stored in a dictionary with key as the tag and \n",
    "    #value as the vector embedding\n",
    "    dict[key]=numbers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a6aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lexeme embeddings- 103055\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of lexeme embeddings-\",len(dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dea5e3",
   "metadata": {},
   "source": [
    "### Load pre-trained word embeddings from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93f7d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model as given in the question\n",
    "# The word embeddings are taken from this model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a4d8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the function words to be removed. These words do not contain word embeddings and\n",
    "# do not have more than one synset.\n",
    "\n",
    "functionwords = ['about', 'across', 'against', 'along', 'around', 'at',\n",
    "                 'behind', 'beside', 'besides', 'by', 'despite', 'down',\n",
    "                 'during', 'for', 'from', 'in', 'inside', 'into', 'near', 'of',\n",
    "                 'off', 'on', 'onto', 'over', 'through', 'to', 'toward',\n",
    "                 'with', 'within', 'without', 'anything', 'everything',\n",
    "                 'anyone', 'everyone', 'ones', 'such', 'it', 'itself',\n",
    "                 'something', 'nothing', 'someone', 'the', 'some', 'this',\n",
    "                 'that', 'every', 'all', 'both', 'one', 'first', 'other',\n",
    "                 'next', 'many', 'much', 'more', 'most', 'several', 'no', 'a',\n",
    "                 'an', 'any', 'each', 'no', 'half', 'twice', 'two', 'second',\n",
    "                 'another', 'last', 'few', 'little', 'less', 'least', 'own',\n",
    "                 'and', 'but', 'after', 'when', 'as', 'because', 'if', 'what',\n",
    "                 'where', 'which', 'how', 'than', 'or', 'so', 'before', 'since',\n",
    "                 'while', 'although', 'though', 'who', 'whose', 'can', 'may',\n",
    "                 'will', 'shall', 'could', 'be', 'do', 'have', 'might', 'would',\n",
    "                 'should', 'must', 'here', 'there', 'now', 'then', 'always',\n",
    "                 'never', 'sometimes', 'usually', 'often', 'therefore',\n",
    "                 'however', 'besides', 'moreover', 'though', 'otherwise',\n",
    "                 'else', 'instead', 'anyway', 'incidentally', 'meanwhile',')','(','{','}','.',';',',',\"'\",\n",
    "                '-',');',\"'s\",'.);','.,']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab0574",
   "metadata": {},
   "source": [
    "## Implement two baseline methods: most common sense and the plain Lesk algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f9a0e",
   "metadata": {},
   "source": [
    "### Most common sense algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c4b8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing\n",
    "predictions = [] \n",
    "targets = []\n",
    "\n",
    "#Function to return the most frequent sense for a lemma\n",
    "def baseline(lemma):\n",
    "    synset = wordnet.synsets(lemma)\n",
    "    if len(synset) > 0:\n",
    "        return wordnet.synsets(lemma)[0] \n",
    "    # First synset is the most common one\n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76421590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return the total number of words taken and the total number of correct senses matched\n",
    "#Input to function - a sentence and its sub dictionary containing all the words and phrases\n",
    "\n",
    "def most_common_sense(sentence,data_dict):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    \n",
    "    #Tokenize the sentence to get words\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #Iterating through each word\n",
    "    for tok in tokens:\n",
    "        \n",
    "        #Getting the most common synset\n",
    "        pred = baseline(tok)\n",
    "        \n",
    "        #While some synset is returned\n",
    "        if pred is not None:\n",
    "           \n",
    "            #Checking the accuracy\n",
    "            \n",
    "            #Retrieving ground truth for the token\n",
    "            if tok not in data_dict and  tok+'.' in data_dict:\n",
    "                #Adding to solve tokenization error\n",
    "                tok=tok+'.'\n",
    "            \n",
    "            if(tok in data_dict):\n",
    "                #getting the lemma, pos and wnsn\n",
    "                lemma = data_dict[tok].split(',')[0]\n",
    "                pos = data_dict[tok].split(',')[1]\n",
    "                wnsn = data_dict[tok].split(',')[2]\n",
    "                \n",
    "                #Forming the true synset value\n",
    "                true_synset = lemma + '.'+pos+'.'+'0'+wnsn\n",
    "\n",
    "                   \n",
    "                true_synset_arr=true_synset.split(\".\")\n",
    "                predicted_synset_arr=pred.name().split(\".\")\n",
    "            \n",
    "            #Calculating accuracy\n",
    "            \n",
    "                #Some case more than one synset may be the ground truth\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    #Form an array of all the true synsets\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "                \n",
    "                #Checking whether the predicted and true synset is same \n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    \n",
    "                    #Incrementing the count and correct value\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "    \n",
    "    #Returning the number of true matches and total count for a sentence\n",
    "    return correct, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e61c4e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of most common sense for senseval2- 47.11578947368421\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#for each sentence\n",
    "for i in data_dict_senseval2:\n",
    "    \n",
    "    #Find the total true matches and counts for all\n",
    "    correct,count = most_common_sense(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "    \n",
    "#Accuracy calculation\n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of most common sense for senseval2-\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a3cc0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of most common sense for senseval3- 45.96923076923077\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation\n",
    "\n",
    "#Same steps as done for senseval2\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval2:\n",
    "    correct,count = most_common_sense(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of most common sense for senseval3-\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f50d5fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of most common sense for 5000 sentences in semcor- 48.235331592023954\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation for 5000 random sentences\n",
    "\n",
    "#Same steps as done for senseval2\n",
    "correct =0\n",
    "count = 0\n",
    "\n",
    "#Take 5000 sentences\n",
    "final_arr = []\n",
    "\n",
    "for i in index_list:\n",
    "    \n",
    "    correct,count = most_common_sense(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of most common sense for 5000 sentences in semcor-\", accuracy * 100)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787584f2",
   "metadata": {},
   "source": [
    "## Plain Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a2a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the number of overlapping words\n",
    "#Input- synset and the sentence being considered\n",
    "def overlap_match( synset, sentence ):\n",
    "    \n",
    "    #Finding the words in the synset definition\n",
    "    gloss = set(WordPunctTokenizer().tokenize(synset.definition()))\n",
    "    \n",
    "    #Finding the words in the synset examples and adding them to gloss\n",
    "    for i in synset.examples():\n",
    "         gloss.union(i)\n",
    "            \n",
    "    #Removing the function words from gloss\n",
    "    gloss = gloss.difference( functionwords )\n",
    "    \n",
    "    #Removing the function words from the sentence\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = set(sentence.split(\" \"))\n",
    "    elif isinstance(sentence, list):\n",
    "        sentence = set(sentence)\n",
    "    elif isinstance(sentence, set):\n",
    "        pass\n",
    "    else:\n",
    "        return\n",
    "    sentence = sentence.difference( functionwords )\n",
    "    \n",
    "    #Returning the number of common words between sentence and gloss\n",
    "    return len( gloss.intersection(sentence) )\n",
    "\n",
    "#Function to return the best synset for each word in a sentence\n",
    "#Input- word and its sentence\n",
    "def lesk( word, sentence ):\n",
    "    bestsense = None\n",
    "    maxoverlap = 0\n",
    "    \n",
    "    #get the word\n",
    "    word=wordnet.morphy(word) if wordnet.morphy(word) is not None else word\n",
    "    \n",
    "    #For each sense of the word in wordnet\n",
    "    for sense in wordnet.synsets(word):\n",
    "        \n",
    "        #Find its overlap number\n",
    "        overlap = overlap_match(sense,sentence)\n",
    "        \n",
    "        #for each hyponyms in sense, find overlap number and add it\n",
    "        for h in sense.hyponyms():\n",
    "            overlap += overlap_match( h, sentence )\n",
    "        \n",
    "        #finding the maximum overlapping sense\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense\n",
    "    \n",
    "    #sense having maximum overlap is returned\n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c151e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing plain lesk algorithm \n",
    "#Input - each sentence and its sub dictionary containing its words and phrases\n",
    "def plain_lesk(sentence, data_dict):\n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    \n",
    "    #tokenize the sentence\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #For each word in sentence\n",
    "    for tok in tokens:\n",
    "        \n",
    "        #Find best synset \n",
    "        best_synset = lesk(tok,sentence)\n",
    "        \n",
    "        #if some best synset is returned\n",
    "        if best_synset is not None:\n",
    "            \n",
    "            #Finding the true synset from the sub dictionary\n",
    "            lemma = data_dict[tok].split(',')[0]\n",
    "            pos = data_dict[tok].split(',')[1]\n",
    "            wnsn = data_dict[tok].split(',')[2]\n",
    "            true_synset = lemma + '.'+pos+'.'+'0'+wnsn\n",
    "            \n",
    "            \n",
    "            true_synset_arr=true_synset.split(\".\")\n",
    "            predicted_synset_arr=best_synset.name().split(\".\")\n",
    "            \n",
    "    #Calculating accuracy\n",
    "            \n",
    "            \n",
    "           #If there are more than one true synset, then list is made\n",
    "            if ';' in true_synset_arr[2]:\n",
    "                true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "            else:\n",
    "                true_synset_arr_1=true_synset_arr[2]\n",
    "                \n",
    "            #checking if predicted and true synsets are same or not\n",
    "            if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                \n",
    "                #increment counters\n",
    "                correct+=1\n",
    "                count+=1\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                count+=1\n",
    "                \n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    #Return the number of correct matches and the total number  \n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "982af94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plain lesk for senseval2 - 43.444365698086465\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation for plain lesk\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#For each sentence in senseval2\n",
    "for i in data_dict_senseval2:\n",
    "    \n",
    "    #Find the total number of correct matches and the total number of words \n",
    "    correct,count = plain_lesk(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "\n",
    "#Final accuracy\n",
    "accuracy = correct_total/count_total\n",
    "\n",
    "\n",
    "print(\"Accuracy of plain lesk for senseval2 -\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac9f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plain lesk for senseval3 - 37.911646586345384\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation for plain lesk\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#For each sentence in senseval3\n",
    "for i in data_dict_senseval3:\n",
    "    \n",
    "     #Find the total number of correct matches and the total number of words \n",
    "    correct,count = plain_lesk(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "\n",
    "#Final accuracy\n",
    "accuracy = correct_total/count_total\n",
    "\n",
    "\n",
    "print(\"Accuracy of plain lesk for senseval3 -\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c3362f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plain lesk for semcor - 38.91576799766832\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation for plain lesk\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#For each of the selected random 5000 sentences in semcor\n",
    "for i in index_list:\n",
    "    \n",
    "     #Find the total number of correct matches and the total number of words \n",
    "    correct,count = plain_lesk(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "\n",
    "#Final accuracy    \n",
    "accuracy = correct_total/count_total\n",
    "\n",
    "\n",
    "print(\"Accuracy of plain lesk for semcor -\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa058f",
   "metadata": {},
   "source": [
    "## Implement the method proposed by Oele and van Noord (2017) using the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69d1276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to implement the baseline paper\n",
    "#Input - sentence and its sub dictionary\n",
    "\n",
    "def fun_lesk(sentence,sub_dict):\n",
    "    \n",
    "    #Initialization\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    score_list = []\n",
    "    disambiguated = {}\n",
    "    \n",
    "    #get all words in a sentence\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #Sort the words in increasing order of number of synsets\n",
    "    sorted_words = []\n",
    "    for w in tokens:\n",
    "        if len(wordnet.synsets(w))>0:\n",
    "            sorted_words.append([w,len(wordnet.synsets(w))])\n",
    "    \n",
    "    #Sorted_words contain the words in the sentence in increasing order \n",
    "    #of their number of synsets available as described in the paper\n",
    "    sorted_words.sort(key = lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    #for each word in the sorted list\n",
    "    for word in sorted_words:\n",
    "        word = word[0]\n",
    "        \n",
    "        #for each sense of the word\n",
    "        for sense in wordnet.synsets(word):\n",
    "            \n",
    "            \n",
    "        #Finding gloss vector for each synset of a word\n",
    "            #Adding the words in the definition of synset\n",
    "            gloss = set(WordPunctTokenizer().tokenize(sense.definition()))\n",
    "            \n",
    "            #adding the words in the examples of the synset\n",
    "            for ex in sense.examples():\n",
    "                tok = set(word_tokenize(ex))\n",
    "                gloss = gloss.union(tok)\n",
    "            \n",
    "            #Now gloss contains all the words in the synset definition and example\n",
    "            #removing the function words\n",
    "            gloss = gloss.difference(functionwords)    \n",
    "            \n",
    "           \n",
    "            embedd = []\n",
    "            cont_emb = []\n",
    "            context_tokens = []\n",
    "            \n",
    "            #for each word in gloss, we find its word embedding and append it\n",
    "            for i in gloss:\n",
    "                if i in model.key_to_index:\n",
    "                    embedd.append(model[i])\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            #Gloss vector - average of all the word embeddings in the gloss\n",
    "            #Gloss vector is a 300 vector size embedding\n",
    "            gloss_vec = [sum(vals)/len(embedd) for vals in zip(*embedd)]  \n",
    "\n",
    "        \n",
    "        #To get context embedding for the corresponding word\n",
    "            \n",
    "            #tokenize the given sentence and remove the word which is being considered\n",
    "            #we remove the function words also\n",
    "            #context will contain all the other words in the sentence\n",
    "            sen_tokens = set(word_tokenize(sentence))\n",
    "            sen_tokens = sen_tokens.difference(functionwords)\n",
    "            \n",
    "            \n",
    "            #append all the context words to an array\n",
    "            for i in sen_tokens:\n",
    "                if(i!= word):\n",
    "                    context_tokens.append(i)\n",
    "           \n",
    "            #for each word in context, find its embedding\n",
    "            for i in context_tokens :\n",
    "                \n",
    "                #if word already has a predicted synset,\n",
    "                #if the word in the context was considered previously, then we take\n",
    "                #the predicted synset's gloss embedding - given in paper\n",
    "                \n",
    "                if i in disambiguated:\n",
    "                    #check the predicted synset value of the word\n",
    "                    pred = disambiguated[i]\n",
    "                    \n",
    "                #find gloss embedding\n",
    "                    #definition words\n",
    "                    new_gloss = set(WordPunctTokenizer().tokenize(pred.definition()))\n",
    "                    \n",
    "                    #example words\n",
    "                    for ex in pred.examples():\n",
    "                        tok = set(word_tokenize(ex))\n",
    "                        new_gloss = new_gloss.union(tok)\n",
    "            \n",
    "            \n",
    "                    new_gloss = new_gloss.difference(functionwords)    \n",
    "                    \n",
    "                    #for each word in gloss, find the word embedding if it is present in the \n",
    "                    #word2vec model and append all embeddings to an array\n",
    "                    for k in new_gloss:\n",
    "                    \n",
    "                        if k in model.key_to_index:\n",
    "    \n",
    "                            cont_emb.append(model[k])\n",
    "\n",
    "                \n",
    "            #if word has not been disambiguated yet, then just take its word embedding from word2vec model \n",
    "                else:\n",
    "                    if i in model.key_to_index:\n",
    "                        cont_emb.append(model[i])\n",
    "\n",
    "            \n",
    "            #context vector for the given word\n",
    "            #find average of the array storing context embeddings\n",
    "            context_vec = [sum(vals)/len(cont_emb) for vals in zip(*cont_emb)]   \n",
    "\n",
    "            \n",
    "            #Score1\n",
    "            #cosine of gloss and context vectors - first part\n",
    "            if(len(gloss)== 0 or len(cont_emb)== 0 or len(embedd)==0): #check for null\n",
    "            \n",
    "                sim1 = 0\n",
    "                \n",
    "            else:\n",
    "                sim1 = distance.cosine(gloss_vec,context_vec)\n",
    "    \n",
    "    \n",
    "        #Find Lexeme embedding of word\n",
    "            if(word in sub_dict):\n",
    "                \n",
    "                #get lexeme format from sub dict\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "                format = lemma+\"-wn-2.1-\"+str(sense.offset()).zfill(8)+\"-\"+pos\n",
    "                \n",
    "\n",
    "                #extract corresponding lexeme embedding from lexeme.txt file\n",
    "                # which is stored as dictionary dict with key and value\n",
    "                if((format in dict) and (len(cont_emb)!=0)): #check null\n",
    "                    lexeme_embed = dict[format]\n",
    "\n",
    "                    #Score2\n",
    "                    #cosine of lexeme embeddings and context vector\n",
    "                    sim2 = distance.cosine(lexeme_embed,context_vec)\n",
    "                else:\n",
    "                    sim2 = 0\n",
    "            else:\n",
    "                    sim2 = 0\n",
    "            \n",
    "            \n",
    "            # score value - from paper\n",
    "            score = sim1 + sim2 \n",
    "            \n",
    "            #append the scores of all senses of the word to a list\n",
    "            score_list.append(score)\n",
    "         \n",
    "        #If more than one synset is there for a word, find the best score\n",
    "        if(len(wordnet.synsets(word))!=0):\n",
    "            \n",
    "            #Finding index of max score and finding synset of word in that index\n",
    "            max_score_index = np.argmax(score_list)\n",
    "            pred_synset = wordnet.synsets(word)[max_score_index]\n",
    "            \n",
    "            #storing the predicted synset for future purposes (context embedding)\n",
    "            disambiguated[word] = pred_synset\n",
    "            \n",
    "           \n",
    "            \n",
    "            if word not in sub_dict and  word+'.' in sub_dict: #to solve error in tokenization\n",
    "                word=word+'.'\n",
    "            \n",
    "            #if the corresponding word occurs in sub dictionary\n",
    "            if word in sub_dict:\n",
    "                \n",
    "                #Finding the true synset value from the phrase (wnsn)\n",
    "                wnsn = sub_dict[word].split(',')[2]\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "\n",
    "                true_syn = lemma+\".\"+pos+\".0\"+wnsn\n",
    "               \n",
    "                true_synset_arr=true_syn.split(\".\")\n",
    "                predicted_synset_arr=pred_synset.name().split(\".\")\n",
    "\n",
    "\n",
    "            # Sometimes more than one true synset occurs, then store in list\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "\n",
    "            #Check whether predicted and true synset values are same or not\n",
    "            #update counters accordingly\n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                    \n",
    "                score_list = []\n",
    "            else:\n",
    "                score_list = []\n",
    "        else:\n",
    "            score_list = []\n",
    "        \n",
    "    \n",
    "#return the number of correct matches and the total count of words considered in the sentence\n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f6ad8",
   "metadata": {},
   "source": [
    "## Evaluate the method on the given three datasets\n",
    "\n",
    "#### Evaluation is similar to what was done for plain lesk and most common sense, only difference is the function being called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52ebc988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributional Lesk for senseval2- 36.46315789473684\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#for each sentence in dataset\n",
    "for i in data_dict_senseval2:\n",
    "    \n",
    "    correct,count = fun_lesk(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributional Lesk for senseval2-\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da8ba2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributional Lesk for senseval3- 37.1973209685729\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#for each sentence in dataset\n",
    "for i in data_dict_senseval3:\n",
    "    \n",
    "    correct,count = fun_lesk(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "\n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributional Lesk for senseval3-\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "312192e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributional Lesk for semcor - 36.907521246934735\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation\n",
    "\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#for each of the 5000 sentences selected\n",
    "for i in index_list:\n",
    "    correct,count = fun_lesk(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    correct_total +=correct\n",
    "    count_total +=count\n",
    "accuracy = correct_total/count_total\n",
    "\n",
    "\n",
    "print(\"Accuracy of Distributional Lesk for semcor -\", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98228027",
   "metadata": {},
   "source": [
    "# Extensions Implementation\n",
    "\n",
    "\n",
    "## 1. Experiment with removing stopwords and punctuation from the dictionary glosses, sense descriptions and contexts in the occurrences of the words before measuring the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98bcf97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Import nltk stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#store stopwords in a list\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2f82542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stopwords and punctuation from gloss, sense and context embeddings.\n",
    "\n",
    "#This function is similar to the baseline function, only difference is that\n",
    "# stop words are being removed before finding each of the embeddings\n",
    "#Input and output of the function remains the same\n",
    "def fun_lesk_stopwords(sentence,sub_dict):\n",
    "    \n",
    "    #get all words in a sentence\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    score_list = []\n",
    "    disambiguated = {}\n",
    "    sorted_words = []\n",
    "    \n",
    "    #tokenize the words and remove the stopwords\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    tokens = tokens.difference(stopwords_list)\n",
    "    \n",
    "    #tokens contain no stopwords\n",
    "    #Sort the words in increasing order of number of synsets    \n",
    "    for w in tokens:\n",
    "        if len(wordnet.synsets(w))>0:\n",
    "            sorted_words.append([w,len(wordnet.synsets(w))])\n",
    "\n",
    "    sorted_words.sort(key = lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    #for each word\n",
    "    for word in sorted_words:\n",
    "        word = word[0]\n",
    "        \n",
    "        #for each sense of the word\n",
    "        for sense in wordnet.synsets(word):\n",
    "           \n",
    "        #Finding gloss vector for each synset of a word\n",
    "            #gloss definition\n",
    "            gloss = set(WordPunctTokenizer().tokenize(sense.definition()))\n",
    "            #gloss examples\n",
    "            for ex in sense.examples():\n",
    "                tok = set(word_tokenize(ex))\n",
    "                gloss = gloss.union(tok)\n",
    "            \n",
    "            #remove stopwords and function words from gloss\n",
    "            gloss = gloss.difference(functionwords)  \n",
    "            gloss = gloss.difference(stopwords_list)\n",
    "            \n",
    "            embedd = []\n",
    "            cont_emb = []\n",
    "            context_tokens = []\n",
    "            \n",
    "            #append word embeddings for all words in gloss\n",
    "            for i in gloss:\n",
    "                if i in model.key_to_index:\n",
    "                    embedd.append(model[i])\n",
    "                else:\n",
    "                       pass\n",
    "             \n",
    "            #Gloss vec- avg of all word embeddings in gloss\n",
    "            gloss_vec = [sum(vals)/len(embedd) for vals in zip(*embedd)]  \n",
    "\n",
    "        #To get context embedding for the corresponding word\n",
    "            \n",
    "            #Tokenize sentence and take all the words except the current word\n",
    "            sen_tokens = set(word_tokenize(sentence))\n",
    "            sen_tokens = sen_tokens.difference(functionwords)\n",
    "            \n",
    "            #remove stop words from context\n",
    "            sen_tokens = sen_tokens.difference(stopwords_list)\n",
    "            for i in sen_tokens:\n",
    "                if(i!= word):\n",
    "                    context_tokens.append(i)\n",
    "            \n",
    "            #for all context words\n",
    "            for i in context_tokens :\n",
    "                \n",
    "                #if word already has a predicted synset\n",
    "                if i in disambiguated:\n",
    "                    \n",
    "                    #find predicted sense \n",
    "                    pred = disambiguated[i]\n",
    "                    \n",
    "                    #Find gloss for predicted synset\n",
    "                    new_gloss = set(WordPunctTokenizer().tokenize(pred.definition()))\n",
    "                    for ex in pred.examples():\n",
    "                        tok = set(word_tokenize(ex))\n",
    "                        new_gloss = new_gloss.union(tok)\n",
    "            \n",
    "            \n",
    "                    new_gloss = new_gloss.difference(functionwords) \n",
    "                #removing stopwords here also\n",
    "                    new_gloss = new_gloss.difference(stopwords_list)\n",
    "                    \n",
    "                    #finding word embeddings for all words in gloss and appending\n",
    "                    for k in new_gloss:\n",
    "                        if k in model.key_to_index:\n",
    "                            cont_emb.append(model[k])\n",
    "\n",
    "                    \n",
    "                #word is not disambiguated, find its word embedding \n",
    "                else:\n",
    "                    if i in model.key_to_index:\n",
    "                        cont_emb.append(model[i])\n",
    "\n",
    "            #context vector for the given word - average of context embeddings\n",
    "            context_vec = [sum(vals)/len(cont_emb) for vals in zip(*cont_emb)]   \n",
    "\n",
    "            #Score 1\n",
    "            #cosine of gloss and context vectors - first part\n",
    "            if(len(gloss)== 0 or len(cont_emb)== 0 or len(embedd)==0):#nullcheck\n",
    "            \n",
    "                sim1 = 0\n",
    "                \n",
    "            else:\n",
    "                sim1 = distance.cosine(gloss_vec,context_vec)\n",
    "    \n",
    "            if(word in sub_dict):\n",
    "            #Getting lexeme embeddings - similar to baseline (no change)\n",
    "                #lexeme format from sub dict\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "                format = lemma+\"-wn-2.1-\"+str(sense.offset()).zfill(8)+\"-\"+pos\n",
    "                \n",
    "\n",
    "                #extract corresponding lexeme embedding from lexeme.txt file\n",
    "                # which isstored as dictionary dict with key and value\n",
    "                if((format in dict) and (len(cont_emb)!=0)): #null check\n",
    "                    lexeme_embed = dict[format]\n",
    "\n",
    "                    #Score 2\n",
    "                    #cosine of lexeme embeddings and context vector\n",
    "                    sim2 = distance.cosine(lexeme_embed,context_vec)\n",
    "                else:\n",
    "                    sim2 = 0\n",
    "            else:\n",
    "                    sim2 = 0\n",
    "            \n",
    "            # score value\n",
    "            score = sim1 + sim2 \n",
    "           \n",
    "            score_list.append(score)\n",
    "           \n",
    "        #Finding maximum score among all synsets\n",
    "        if(len(wordnet.synsets(word))!=0):\n",
    "            \n",
    "            #Finding predicted synset and storing it\n",
    "            max_score_index = np.argmax(score_list) \n",
    "            pred_synset = wordnet.synsets(word)[max_score_index]\n",
    "\n",
    "            disambiguated[word] = pred_synset\n",
    "            \n",
    "                \n",
    "            if word not in sub_dict and  word+'.' in sub_dict:\n",
    "                word=word+'.'\n",
    "            \n",
    "            #True synset calculation\n",
    "            if word in sub_dict:\n",
    "                wnsn = sub_dict[word].split(',')[2]\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "\n",
    "                true_syn = lemma+\".\"+pos+\".0\"+wnsn\n",
    "\n",
    "\n",
    "                true_synset_arr=true_syn.split(\".\")\n",
    "                predicted_synset_arr=pred_synset.name().split(\".\")\n",
    "\n",
    "\n",
    "                #When more than one true synset is present\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "\n",
    "                #comparison of true and predicted synset\n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                   \n",
    "                score_list = []\n",
    "            else:\n",
    "                score_list = []\n",
    "        else:\n",
    "            score_list = []\n",
    "        \n",
    "    \n",
    "    #return the matching and total count    \n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b847b",
   "metadata": {},
   "source": [
    "### Evaluation on 3 datasets\n",
    "\n",
    "#### Evaluation is the same method for all datasets as seen in baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d995048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after removing stopwords on senseval2 - 35.69804456571169\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation - stopwords\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval2:\n",
    "    correct,count = fun_lesk_stopwords(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy after removing stopwords on senseval2 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc944464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after removing stopwords on senseval3 - 36.348219332956475\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation - stopwords\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval3:\n",
    "    correct,count = fun_lesk_stopwords(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy after removing stopwords on senseval3 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c1a4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after removing stopwords on semcor - 36.70728015740881\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation - stopwords\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "#Take 5000 sentences\n",
    "for i in index_list:\n",
    "    correct,count = fun_lesk_stopwords(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy after removing stopwords on semcor -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe34e5",
   "metadata": {},
   "source": [
    "### 2. SemCor data come from the Brown corpus. The Brown corpus consists of texts from different text categories (see e.g. https://www1.essex.ac.uk/ linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html). Evaluate the results for individual categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4b37c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36']\n",
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44']\n",
      "['cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29']\n"
     ]
    }
   ],
   "source": [
    "#Import the Brown corpus from nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#To get all the brown categories\n",
    "brown.categories()\n",
    "\n",
    "#After trial and error, categories having most number of sentences-\n",
    "print(brown.fileids(['hobbies']))\n",
    "print(brown.fileids(['news']))\n",
    "print(brown.fileids(['adventure']))\n",
    "\n",
    "rootdir = 'semcor1.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9ac92a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "75\n",
      "['cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.fileids(['belles_lettres'])))\n",
    "print(brown.fileids(['belles_lettres']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70eb32af",
   "metadata": {},
   "source": [
    "#### Reading files in category belles_lettres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b6e3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files in category belles_lettres\n",
    "\n",
    "#storing required file paths\n",
    "arr_sem_cor_file_belle=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #found from documentation\n",
    "        if 'br-g' in file:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_sem_cor_file_belle.append(var1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63b810db",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "data_dict_semcor_belle = {}\n",
    "\n",
    "#Reading and storing sentences in belle category files\n",
    "\n",
    "#Steps to read and store sentences and subdictionary containing words in sentence and its phrase\n",
    "#is same as done in earlier cases - for baseline\n",
    "#only the file paths change - corresponding category only\n",
    "for file in arr_sem_cor_file_belle:\n",
    "    \n",
    "    with open(file) as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    \n",
    "    tags = soup.find_all(\"s\")\n",
    "    for i in tags:\n",
    "        tag=i.find_all(\"wf\", {\"cmd\":\"done\"})\n",
    "        \n",
    "        sen1=\"\"\n",
    "        dict3={}\n",
    "        for j in tag:\n",
    "            if type(j.get(\"lemma\"))==str:\n",
    "                flag=True\n",
    "                if (not (j.get(\"ot\"))):\n",
    "                    sen1+=j.text\n",
    "                    sen1+=\" \"\n",
    "                    key=j.text\n",
    "                    if j.get(\"pos\") in dict_postags.keys():\n",
    "                        pos=dict_postags[j.get(\"pos\")]\n",
    "                    else:\n",
    "                        print(\"Pos not available\")\n",
    "                    value=j.get(\"lemma\")+\",\"+pos+\",\"+j.get(\"wnsn\")\n",
    "                    dict3[key]=value\n",
    "            else:\n",
    "                flag=False\n",
    "        \n",
    "        if flag:\n",
    "            key=counter\n",
    "            value=sen1\n",
    "            data_dict_semcor_belle[key]=[value,dict3]\n",
    "            counter+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e72f749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Belles_lettres semcor category- 2340\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in Belles_lettres semcor category-\",len(data_dict_semcor_belle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbf66d2",
   "metadata": {},
   "source": [
    "#### Reading files in category news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f4390de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files in category News\n",
    "\n",
    "#storing required file paths\n",
    "arr_sem_cor_file_news=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        #found from documentation\n",
    "        if 'br-a' in file:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_sem_cor_file_news.append(var1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1a23de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "data_dict_semcor_news = {}\n",
    "\n",
    "#Reading and storing sentences in news category files\n",
    "\n",
    "#Steps to read and store sentences and subdictionary containing words in sentence and its phrase\n",
    "#is same as done in earlier cases - for baseline\n",
    "#only the file paths change - corresponding category only\n",
    "for file in arr_sem_cor_file_news:\n",
    "    \n",
    "    with open(file) as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    \n",
    "    tags = soup.find_all(\"s\")\n",
    "    for i in tags:\n",
    "        tag=i.find_all(\"wf\", {\"cmd\":\"done\"})\n",
    "        \n",
    "        sen1=\"\"\n",
    "        dict3={}\n",
    "        for j in tag:\n",
    "            if type(j.get(\"lemma\"))==str:\n",
    "                flag=True\n",
    "                if (not (j.get(\"ot\"))):\n",
    "                    sen1+=j.text\n",
    "                    sen1+=\" \"\n",
    "                    key=j.text\n",
    "                    if j.get(\"pos\") in dict_postags.keys():\n",
    "                        pos=dict_postags[j.get(\"pos\")]\n",
    "                    else:\n",
    "                        print(\"Pos not available\")\n",
    "                    value=j.get(\"lemma\")+\",\"+pos+\",\"+j.get(\"wnsn\")\n",
    "                    dict3[key]=value\n",
    "            else:\n",
    "                flag=False\n",
    "        \n",
    "        if flag:\n",
    "            key=counter\n",
    "            value=sen1\n",
    "            data_dict_semcor_news[key]=[value,dict3]\n",
    "            counter+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc2e2219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in News semcor category- 3819\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in News semcor category-\",len(data_dict_semcor_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637b2ea",
   "metadata": {},
   "source": [
    "#### Reading files in category hobbies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a350f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files in category Hobbies\n",
    "\n",
    "arr_sem_cor_file_hobbies=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if 'br-e' in file:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_sem_cor_file_hobbies.append(var1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f41d0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "data_dict_semcor_hobbies = {}\n",
    "\n",
    "#Reading and storing sentences in hobbies category files\n",
    "\n",
    "#Steps to read and store sentences and subdictionary containing words in sentence and its phrase\n",
    "#is same as done in earlier cases - for baseline\n",
    "#only the file paths change - corresponding category only\n",
    "\n",
    "for file in arr_sem_cor_file_hobbies:\n",
    "    with open(file) as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    \n",
    "    tags = soup.find_all(\"s\")\n",
    "    \n",
    "    for i in tags:\n",
    "        tag=i.find_all(\"wf\", {\"cmd\":\"done\"})\n",
    "        \n",
    "        sen1=\"\"\n",
    "        dict3={}\n",
    "        for j in tag:\n",
    "            if type(j.get(\"lemma\"))==str:\n",
    "                flag=True\n",
    "                if (not (j.get(\"ot\"))):\n",
    "                    sen1+=j.text\n",
    "                    sen1+=\" \"\n",
    "                    key=j.text\n",
    "                    if j.get(\"pos\") in dict_postags.keys():\n",
    "                        pos=dict_postags[j.get(\"pos\")]\n",
    "                    else:\n",
    "                        print(\"Pos not available\")\n",
    "                    value=j.get(\"lemma\")+\",\"+pos+\",\"+j.get(\"wnsn\")\n",
    "                    dict3[key]=value\n",
    "            else:\n",
    "                flag=False\n",
    "        \n",
    "        if flag:\n",
    "            key=counter\n",
    "            value=sen1\n",
    "            data_dict_semcor_hobbies[key]=[value,dict3]\n",
    "            counter+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba1ef81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Hobbies semcor category- 2963\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in Hobbies semcor category-\",len(data_dict_semcor_hobbies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94389a75",
   "metadata": {},
   "source": [
    "#### Reading files in category adventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "011db91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files in category adventure\n",
    "\n",
    "arr_sem_cor_file_adv=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if 'br-n' in file:\n",
    "            var1=str(os.path.join(subdir, file))\n",
    "            arr_sem_cor_file_adv.append(var1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9b41550",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "data_dict_semcor_adv = {}\n",
    "\n",
    "#Reading and storing sentences in adventure category files\n",
    "\n",
    "#Steps to read and store sentences and subdictionary containing words in sentence and its phrase\n",
    "#is same as done in earlier cases - for baseline\n",
    "#only the file paths change - corresponding category only\n",
    "\n",
    "for file in arr_sem_cor_file_adv:\n",
    "    with open(file) as fp:\n",
    "        soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    \n",
    "    tags = soup.find_all(\"s\")\n",
    "    \n",
    "    for i in tags:\n",
    "        tag=i.find_all(\"wf\", {\"cmd\":\"done\"})\n",
    "        \n",
    "        sen1=\"\"\n",
    "        dict3={}\n",
    "        for j in tag:\n",
    "            if type(j.get(\"lemma\"))==str:\n",
    "                flag=True\n",
    "                if (not (j.get(\"ot\"))):\n",
    "                    sen1+=j.text\n",
    "                    sen1+=\" \"\n",
    "                    key=j.text\n",
    "                    if j.get(\"pos\") in dict_postags.keys():\n",
    "                        pos=dict_postags[j.get(\"pos\")]\n",
    "                    else:\n",
    "                        print(\"Pos not available\")\n",
    "                    value=j.get(\"lemma\")+\",\"+pos+\",\"+j.get(\"wnsn\")\n",
    "                    dict3[key]=value\n",
    "            else:\n",
    "                flag=False\n",
    "        \n",
    "        if flag:\n",
    "            key=counter\n",
    "            value=sen1\n",
    "            data_dict_semcor_adv[key]=[value,dict3]\n",
    "            counter+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3a3b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Adventure semcor category- 2592\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences in Adventure semcor category-\",len(data_dict_semcor_adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7440c3",
   "metadata": {},
   "source": [
    "### Evaluation on News category\n",
    "\n",
    "#### Steps for evaluation are same as done in baseline, the same baseline function is called here\n",
    "#### The dataset taken is restricted to news category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eba93d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributational lesk on News category - 34.03571695434506\n"
     ]
    }
   ],
   "source": [
    "#Semcor news computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_semcor_news:\n",
    "    \n",
    "    correct,count,acc_list = fun_lesk(data_dict_semcor_news[i][0],data_dict_semcor_news[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributational lesk on News category -\",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083bd16",
   "metadata": {},
   "source": [
    "### Evaluation on Hobbies category\n",
    "\n",
    "#### Steps for evaluation are same as done in baseline, the same baseline function is called here\n",
    "#### The dataset taken is restricted to hobbies category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec6d1166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributational lesk on Hobbies category - 35.78947368421053\n"
     ]
    }
   ],
   "source": [
    "#Semcor hobbies computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_semcor_hobbies:\n",
    "    \n",
    "    correct,count,acc_list = fun_lesk(data_dict_semcor_hobbies[i][0],data_dict_semcor_hobbies[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributational lesk on Hobbies category -\",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3826b7",
   "metadata": {},
   "source": [
    "### Evaluation on Adventure category\n",
    "\n",
    "\n",
    "#### Steps for evaluation are same as done in baseline, the same baseline function is called here\n",
    "#### The dataset taken is restricted to adventure category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32eff0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributational lesk on Adventure category - 37.45493871665465\n"
     ]
    }
   ],
   "source": [
    "#Semcor Adventure computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_semcor_adv:\n",
    "    \n",
    "    correct,count,acc_list = fun_lesk(data_dict_semcor_adv[i][0],data_dict_semcor_adv[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributational lesk on Adventure category -\",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b7945",
   "metadata": {},
   "source": [
    "### Evaluation on Belles_Lettres category\n",
    "\n",
    "\n",
    "#### Steps for evaluation are same as done in baseline, the same baseline function is called here\n",
    "#### The dataset taken is restricted to belles_lettres category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "688bc39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Distributational lesk on Belle_lettres category - 37.42560328968727\n"
     ]
    }
   ],
   "source": [
    "#Semcor Belles_lettres computation\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_semcor_belle:\n",
    "    \n",
    "    correct,count = fun_lesk(data_dict_semcor_belle[i][0],data_dict_semcor_belle[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy of Distributational lesk on Belle_lettres category -\",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9b207",
   "metadata": {},
   "source": [
    "## 3. Train your own word embeddings for this task, possibly initializing the embeddings with pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8536010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#Word2vec model of vector_size 300, window size 3 and minimum count 1 is taken\n",
    "\n",
    "model1 = gensim.models.Word2Vec(min_count=1,\n",
    "                                   window=3,\n",
    "                                   vector_size=300, \n",
    "                                   workers=cores,\n",
    "                                   sg=1\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee012f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##Building Vocab for training our own model by combining all the datasets\n",
    "dict_vocab_list=[data_dict_semcor,data_dict_senseval2,data_dict_senseval3]\n",
    "sentences_full=[]\n",
    "\n",
    "for dict_item in dict_vocab_list:\n",
    "    for j in dict_item.keys():\n",
    "        sentences_full.append(word_tokenize(dict_item[j][0]))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "be23cb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/75yp7_695hg75lbf0tpmjs9w0000gn/T/ipykernel_2003/432324681.py:10: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model1.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# defining the vocabulary based on our data\n",
    "\n",
    "#data for vocab is an array of tokenized sentences\n",
    "model1.build_vocab(sentences_full, progress_per=10000)\n",
    "\n",
    "\n",
    "\n",
    "# training the model\n",
    "model1.train(sentences_full, total_examples=model1.corpus_count, epochs=10, report_delay=1)\n",
    "model1.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "202a54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extending the baseline function to handle our trained model. Here word embeddings are accesed by\n",
    "# model1.wv\n",
    "\n",
    "#Rest all steps remain the same\n",
    "def fun_lesk_own_embeddings(sentence,sub_dict):\n",
    "    \n",
    "    #get all words in a sentence\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    score_list = []\n",
    "    sorted_words = []\n",
    "    disambiguated = {}\n",
    "    \n",
    "    #tokenize sentence\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #Sort the words in increasing order of number of synsets  \n",
    "    for w in tokens:\n",
    "        if len(wordnet.synsets(w))>0:\n",
    "            sorted_words.append([w,len(wordnet.synsets(w))])\n",
    "\n",
    "    sorted_words.sort(key = lambda x: x[1])\n",
    "    \n",
    "    #for each word\n",
    "    for word in sorted_words:\n",
    "        word = word[0]\n",
    "        \n",
    "        #for each sense of the word       \n",
    "        for sense in wordnet.synsets(word):\n",
    "            \n",
    "            \n",
    "            #Finding gloss vector for each synset of a word\n",
    "            #gloss - definition and example words\n",
    "            gloss = set(WordPunctTokenizer().tokenize(sense.definition()))\n",
    "            \n",
    "            for ex in sense.examples():\n",
    "                tok = set(word_tokenize(ex))\n",
    "                gloss = gloss.union(tok)\n",
    "            \n",
    "            gloss = gloss.difference(functionwords)    \n",
    "            \n",
    "\n",
    "            embedd = []\n",
    "            cont_emb = []\n",
    "            context_tokens = []\n",
    "            \n",
    "            #Find newly trained model embeddings for the words in gloss\n",
    "            for i in gloss:\n",
    "                if i in model1.wv.key_to_index:\n",
    "                    embedd.append(model1.wv[i])\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            #gloss vector calculation\n",
    "            gloss_vec = [sum(vals)/len(embedd) for vals in zip(*embedd)]  \n",
    "\n",
    "            #To get context embedding for the corresponding word\n",
    "\n",
    "            sen_tokens = set(word_tokenize(sentence))\n",
    "            sen_tokens = sen_tokens.difference(functionwords)\n",
    "            \n",
    "            #Get context words\n",
    "            for i in sen_tokens:\n",
    "                if(i!= word):\n",
    "                    context_tokens.append(i)\n",
    "            \n",
    "\n",
    "            for i in context_tokens :\n",
    "                \n",
    "                \n",
    "                #if word already has a predicted synset\n",
    "                if i in disambiguated:\n",
    "                    pred = disambiguated[i]\n",
    "                    #find gloss embedding\n",
    "                    new_gloss = set(WordPunctTokenizer().tokenize(pred.definition()))\n",
    "                    for ex in pred.examples():\n",
    "                        tok = set(word_tokenize(ex))\n",
    "                        new_gloss = new_gloss.union(tok)\n",
    "            \n",
    "            \n",
    "                    new_gloss = new_gloss.difference(functionwords)    \n",
    "                    for k in new_gloss:\n",
    "                    #Find embedding from new model\n",
    "                        if k in model1.wv.key_to_index:\n",
    "\n",
    "                            cont_emb.append(model1.wv[k])  \n",
    "                \n",
    "                #if word is not disambiguated, then use new model to get context embedding\n",
    "                else:\n",
    "                    if i in model1.wv.key_to_index:\n",
    "\n",
    "                        cont_emb.append(model1.wv[i])\n",
    "\n",
    "            #context vector for the given word\n",
    "            context_vec = [sum(vals)/len(cont_emb) for vals in zip(*cont_emb)]   \n",
    "\n",
    "            #score1\n",
    "            #cosine of gloss and context vectors - first part\n",
    "            if(len(gloss)== 0 or len(cont_emb)== 0 or len(embedd)==0):#null check\n",
    "            \n",
    "                sim1 = 0\n",
    "                \n",
    "            else:\n",
    "                sim1 = distance.cosine(gloss_vec,context_vec)\n",
    "    \n",
    "    #Get lexeme embeddings - same as baseline\n",
    "            if(word in sub_dict):\n",
    "                #lexeme format from sub dict\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "                format = lemma+\"-wn-2.1-\"+str(sense.offset()).zfill(8)+\"-\"+pos\n",
    "                \n",
    "\n",
    "                #extract corresponding lexeme embedding from lexeme.txt file\n",
    "                # and stored as dictionary dict with key and value\n",
    "                if((format in dict) and (len(cont_emb)!=0)):\n",
    "                    lexeme_embed = dict[format]\n",
    "\n",
    "                    #score2\n",
    "                    #cosine of lexeme embeddings and context vector\n",
    "                    sim2 = distance.cosine(lexeme_embed,context_vec)\n",
    "                else:\n",
    "                    sim2 = 0\n",
    "            else:\n",
    "                    sim2 = 0\n",
    "            # score value\n",
    "            score = sim1 + sim2 \n",
    "            \n",
    "            score_list.append(score)\n",
    "        \n",
    "        #Finding predicted synset\n",
    "        if(len(wordnet.synsets(word))!=0):\n",
    "           \n",
    "            max_score_index = np.argmax(score_list)\n",
    "            pred_synset = wordnet.synsets(word)[max_score_index]\n",
    "            disambiguated[word] = pred_synset\n",
    "            \n",
    "           \n",
    "            if word not in sub_dict and  word+'.' in sub_dict:\n",
    "                word=word+'.'\n",
    "            \n",
    "            #Finding true synset\n",
    "            if word in sub_dict:\n",
    "                \n",
    "                wnsn = sub_dict[word].split(',')[2]\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "\n",
    "                true_syn = lemma+\".\"+pos+\".0\"+wnsn\n",
    "              \n",
    "                true_synset_arr=true_syn.split(\".\")\n",
    "                predicted_synset_arr=pred_synset.name().split(\".\")\n",
    "\n",
    "            #If true synset has more than 1 value, then use list\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "\n",
    "            #Compare predicted and true synset values\n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                    \n",
    "                score_list = []\n",
    "            else:\n",
    "                score_list = []\n",
    "        else:\n",
    "            score_list = []\n",
    "        \n",
    "    #return the number of correct matches and the total count       \n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fa4c6",
   "metadata": {},
   "source": [
    "### Evaluation of datasets\n",
    "\n",
    "#### Evaluation step is same as that for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b17aa191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on senseval2 - 37.68421052631579\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation - own embeddings\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval2:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on senseval2 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dbd3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on senseval3 - 32.045337454920144\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation - own embeddings\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval3:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on senseval3 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5451febf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on semcor - 36.242399811884844\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation - own embeddings\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in index_list:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on semcor -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa5f6b",
   "metadata": {},
   "source": [
    "## 4. Use several pre-trained embeddings or train embeddings with various parameter settings (you probably need to make big changes so you actually get significantly different results for WSD) and study the influence of the used embeddings on the disambiguation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ab8473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I experimented mainly with window size. When I decrease window size, accuracy decreased\n",
    "#On increasing window size, accuracy also increases but only upto a certain point.\n",
    "\n",
    "#Min count and vector size cannot be change as it needs to be compatable with the lexeme embeddings\n",
    "model1 = gensim.models.Word2Vec(min_count=1,\n",
    "                                   window=6,\n",
    "                                   vector_size=300, \n",
    "                                   workers=cores,\n",
    "                                   sg=1\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279b5cb",
   "metadata": {},
   "source": [
    "### Evaluation on 3 datasets\n",
    "\n",
    "#### Only the model used changes, everything else remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a81156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on senseval2 with parameter change - 47.11578947368421\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation - own embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_senseval2:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on senseval2 with parameter change -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9aa146a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on senseval3 with parameter change - 45.69809376609995\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation - own embeddings\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_senseval3:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on senseval3 with parameter change -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca9232e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using our own embeddings on semcor with parameter change- 48.35903120696026\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation - own embeddings\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in index_list:\n",
    "    correct,count = fun_lesk_own_embeddings(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using our own embeddings on semcor with parameter change-\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec90d1",
   "metadata": {},
   "source": [
    "## 5. Extend the word embedding model to also use character-based representations, e.g. fastText or flair embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4cfa7",
   "metadata": {},
   "source": [
    "### Implementing FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a43ecd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import fastText\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "#Specifying the required parameters\n",
    "embedding_size = 300\n",
    "window_size = 6\n",
    "min_word = 1\n",
    "down_sampling = 1e-2\n",
    " \n",
    "#FastText model, with parameters. Train the model on all the sentences in all datasets\n",
    "model_fast = FastText(sentences_full,\n",
    "                      vector_size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = cores,\n",
    "                      sg=1)\n",
    "\n",
    "#Call baseline func using this model \n",
    "\n",
    "# Evaluate on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acd39bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function used is same as baseline function\n",
    "#Only the model used differs for word embeddings\n",
    "\n",
    "def fun_lesk_fastText(sentence,sub_dict):\n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    score_list = []\n",
    "    sorted_words = []\n",
    "    disambiguated = {}\n",
    "    \n",
    "    #get all words in a sentence\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #Sort the words in increasing order of number of synsets \n",
    "    for w in tokens:\n",
    "        if len(wordnet.synsets(w))>0:\n",
    "            sorted_words.append([w,len(wordnet.synsets(w))])\n",
    "\n",
    "    sorted_words.sort(key = lambda x: x[1])\n",
    "    \n",
    "\n",
    "    #for each word\n",
    "    for word in sorted_words:\n",
    "        word = word[0]\n",
    "        \n",
    "        #for each sense of the word\n",
    "        for sense in wordnet.synsets(word):\n",
    "            \n",
    "            #Finding gloss vector for each synset of a word\n",
    "            gloss = set(WordPunctTokenizer().tokenize(sense.definition()))\n",
    "            for ex in sense.examples():\n",
    "                tok = set(word_tokenize(ex))\n",
    "                gloss = gloss.union(tok)\n",
    "             \n",
    "            gloss = gloss.difference(functionwords)    \n",
    "            \n",
    "            context_tokens = []\n",
    "            embedd = []\n",
    "            cont_emb = []\n",
    "            \n",
    "            #Find fastText embeddings for all words in gloss\n",
    "            for i in gloss:\n",
    "                if i in model_fast.wv.key_to_index:\n",
    "                    embedd.append(model_fast.wv[i])\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            #gloss vector calculation\n",
    "            gloss_vec = [sum(vals)/len(embedd) for vals in zip(*embedd)]  \n",
    "\n",
    "            #To get context embedding for the corresponding word\n",
    "\n",
    "            sen_tokens = set(word_tokenize(sentence))\n",
    "            sen_tokens = sen_tokens.difference(functionwords)\n",
    "            \n",
    "            #find context words\n",
    "            for i in sen_tokens:\n",
    "                if(i!= word):\n",
    "                    context_tokens.append(i)\n",
    "           \n",
    "            #for all context words\n",
    "            for i in context_tokens :\n",
    "                \n",
    "                #if word already has a predicted synset\n",
    "                if i in disambiguated:\n",
    "                    pred = disambiguated[i]\n",
    "                    #find gloss embedding\n",
    "                    new_gloss = set(WordPunctTokenizer().tokenize(pred.definition()))\n",
    "                    for ex in pred.examples():\n",
    "                        tok = set(word_tokenize(ex))\n",
    "                        new_gloss = new_gloss.union(tok)\n",
    "            \n",
    "                    new_gloss = new_gloss.difference(functionwords)    \n",
    "                    #find fastText embedding for gloss\n",
    "                    for k in new_gloss: \n",
    "                        if k in model_fast.wv.key_to_index:\n",
    "                            cont_emb.append(model_fast.wv[k])\n",
    "\n",
    "                #if word is not disambiguated, find fastText embedding of word   \n",
    "                else:\n",
    "                    if i in model_fast.wv.key_to_index:\n",
    "\n",
    "                        cont_emb.append(model_fast.wv[i])\n",
    "\n",
    "            #context vector for the given word\n",
    "            context_vec = [sum(vals)/len(cont_emb) for vals in zip(*cont_emb)]   \n",
    "\n",
    "            #cosine of gloss and context vectors - first part\n",
    "            if(len(gloss)== 0 or len(cont_emb)== 0 or len(embedd)==0):#null check\n",
    "                sim1 = 0\n",
    "                \n",
    "            else:\n",
    "                sim1 = distance.cosine(gloss_vec,context_vec)\n",
    "    \n",
    "    #getting lexeme embedding - same as baseline\n",
    "            if(word in sub_dict):\n",
    "                #lexeme format from sub dict\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "                format = lemma+\"-wn-2.1-\"+str(sense.offset()).zfill(8)+\"-\"+pos\n",
    "                \n",
    "\n",
    "                #extract corresponding lexeme embedding from lexeme.txt file\n",
    "                # and stored as dictionary dict with key and value\n",
    "                if((format in dict) and (len(cont_emb)!=0)):\n",
    "                    lexeme_embed = dict[format]\n",
    "\n",
    "                    #cosine of lexeme embeddings and context vector\n",
    "                    sim2 = distance.cosine(lexeme_embed,context_vec)\n",
    "                else:\n",
    "                    sim2 = 0\n",
    "            else:\n",
    "                    sim2 = 0\n",
    "            #score value\n",
    "            score = sim1 + sim2 \n",
    "           \n",
    "            score_list.append(score)\n",
    "           \n",
    "        \n",
    "        if(len(wordnet.synsets(word))!=0):\n",
    "            #Find predicted synset\n",
    "            max_score_index = np.argmax(score_list)\n",
    "            pred_synset = wordnet.synsets(word)[max_score_index]\n",
    "            disambiguated[word] = pred_synset\n",
    "            \n",
    "           \n",
    "            if word not in sub_dict and  word+'.' in sub_dict:\n",
    "                word=word+'.'\n",
    "            \n",
    "            if word in sub_dict:\n",
    "                #Find true synset\n",
    "                wnsn = sub_dict[word].split(',')[2]\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "\n",
    "                true_syn = lemma+\".\"+pos+\".0\"+wnsn\n",
    "               \n",
    "\n",
    "                true_synset_arr=true_syn.split(\".\")\n",
    "                predicted_synset_arr=pred_synset.name().split(\".\")\n",
    "\n",
    "            #If true synset has more than one value - list\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "            \n",
    "            #comparison of predicted and true synsets for accuracy\n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                    \n",
    "                score_list = []\n",
    "            else:\n",
    "                score_list = []\n",
    "        else:\n",
    "            score_list = []\n",
    "        \n",
    "    \n",
    "    #return the number of matching words and total words considered   \n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d226ce",
   "metadata": {},
   "source": [
    "### Evaluation on 3 datasets\n",
    "\n",
    "#### Steps for evaluation remain the same as baseline, only the function called differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c1ebc077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using fastText embeddings on senseval2 - 39.78947368421053\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation - fastText embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_senseval2:\n",
    "    correct,count = fun_lesk_fastText(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using fastText embeddings on senseval2 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f2def14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using fastText embeddings on senseval3 - 33.69397217928903\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation - fastText embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in data_dict_senseval3:\n",
    "    correct,count = fun_lesk_fastText(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using fastText embeddings on senseval3 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5ec273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using fastText embeddings on semcor - 37.159461184453626\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation - fastText embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "for i in index_list:\n",
    "    correct,count = fun_lesk_fastText(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using fastText embeddings on semcor -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c44a8",
   "metadata": {},
   "source": [
    "## 6. Use transformers and sentence embeddings to compare a sentence and a gloss. E.g. you could use the SBERT pre-trained models. Use a part of the data to tune the transformer and classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61be52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a73fcf6e7564af0864aff8f6fa595a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ab68673d674803bba6f8062a59fbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c2c4c1bd0d43fabd575c47f44b4d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17b329468e643098360e5ab62c63f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0a48c8cb04493e84dd468344f2add3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d0e06c38444ac482dad648ce40ab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad9685ecedc451f92e15ed31bd93e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50ae60a65e242c59033e7c2e1077a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b526e1c8783049259a424b0ec188fa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c410c1813246358bbb1385d28e9420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b319de495fa46f8b3b8083d4dc09325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5774a62046d84f3e85b831ea58dcb894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4015d71c96fa447ea63edf5b0e2c4ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755e6f8ef1f143cc81ad91a36dc27ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import required libraries\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "#word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "#pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "#dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=300, activation_function=nn.Tanh())\n",
    "\n",
    "\n",
    "#Sentence embedding pretrained model\n",
    "#model_sbert = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "\n",
    "#Pretrained SBERT embedding model\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c880ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extend baseline to incorporate sentence embeddings in gloss and context using sbert model\n",
    "#Input and output of function remains the same as baseline\n",
    "\n",
    "def fun_lesk_sbert(sentence,sub_dict):\n",
    "    \n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    score_list = []\n",
    "    sorted_words = []\n",
    "    disambiguated = {}\n",
    "    \n",
    "    #get all words in a sentence\n",
    "    tokens = set(word_tokenize(sentence))\n",
    "    \n",
    "    #Sort the words in increasing order of number of synsets\n",
    "    for w in tokens:\n",
    "        if len(wordnet.synsets(w))>0:\n",
    "            sorted_words.append([w,len(wordnet.synsets(w))])\n",
    "\n",
    "    sorted_words.sort(key = lambda x: x[1])\n",
    "    \n",
    " \n",
    "    #for each word\n",
    "    for word in sorted_words:\n",
    "        word = word[0]\n",
    "        \n",
    "        #for each sense of the word\n",
    "        for sense in wordnet.synsets(word):\n",
    "            gloss_arr = []\n",
    "            cont_emb = []\n",
    "            context_tokens = []\n",
    "            \n",
    "            #Finding gloss vector for each synset of a word\n",
    "            \n",
    "            #Sentence embedding for sense definition\n",
    "            gloss_arr.append(model_sbert.encode(sense.definition()))\n",
    "            \n",
    "            #Sentence embedding for sense examples\n",
    "            for ex in sense.examples():\n",
    "                gloss_arr.append(model_sbert.encode(ex))\n",
    "            \n",
    "            #gloss vector calculation - average of the sentence embeddings\n",
    "            gloss_vec = [sum(vals)/len(gloss_arr) for vals in zip(*gloss_arr)]  \n",
    "            \n",
    "            \n",
    "            #To get context embedding for the corresponding word\n",
    "            #Same steps as that of baseline\n",
    "            sen_tokens = set(word_tokenize(sentence))\n",
    "            sen_tokens = sen_tokens.difference(functionwords)\n",
    "            \n",
    "            \n",
    "            #get context tokens\n",
    "            for i in sen_tokens:\n",
    "                if(i!= word):\n",
    "                    context_tokens.append(i)\n",
    "           \n",
    "            \n",
    "            #context embedding for the sentence(only context tokens)\n",
    "            cont_sentence = \"\"\n",
    "            for t in context_tokens:\n",
    "                cont_sentence += t + \" \"\n",
    "            cont_emb_sbert = model_sbert.encode(cont_sentence)\n",
    "\n",
    "              \n",
    "\n",
    "            #cosine of gloss and context vectors of sentences - first part\n",
    "            if(len(gloss_arr)== 0 or len(cont_emb_sbert)== 0): #null check\n",
    "                sim1 = 0\n",
    "               \n",
    "            else:\n",
    "                sim1 = distance.cosine(gloss_vec,cont_emb_sbert)\n",
    "    \n",
    "            \n",
    "        #context vector for the given word same as baseline - for second score\n",
    "            for i in context_tokens :\n",
    "               \n",
    "                #if word already has a predicted synset\n",
    "                if i in disambiguated:\n",
    "                    pred = disambiguated[i]\n",
    "                    \n",
    "                    #find gloss embedding\n",
    "                    new_gloss = set(WordPunctTokenizer().tokenize(pred.definition()))\n",
    "                    for ex in pred.examples():\n",
    "                        tok = set(word_tokenize(ex))\n",
    "                        new_gloss = new_gloss.union(tok)\n",
    "            \n",
    "                    new_gloss = new_gloss.difference(functionwords)    \n",
    "                    #find word embedding for gloss\n",
    "                    for k in new_gloss: \n",
    "                        if k in model.key_to_index:\n",
    "                            cont_emb.append(model[k])\n",
    "                \n",
    "                #if word is not disambiguated, find word embedding from word2vec model   \n",
    "                else:\n",
    "                    if i in model.key_to_index:\n",
    "                        cont_emb.append(model[i])\n",
    "            context_vec = [sum(vals)/len(cont_emb) for vals in zip(*cont_emb)] \n",
    "   \n",
    "        #Get lexeme embedding for word - same as baseline\n",
    "            \n",
    "            if(word in sub_dict):\n",
    "                #lexeme format from sub dict\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "                format = lemma+\"-wn-2.1-\"+str(sense.offset()).zfill(8)+\"-\"+pos\n",
    "                \n",
    "\n",
    "                #extract corresponding lexeme embedding from lexeme.txt file\n",
    "                # and stored as dictionary dict with key and value\n",
    "                if((format in dict) and (len(cont_emb)!=0)):\n",
    "                    lexeme_embed = dict[format]\n",
    "\n",
    "                    #cosine of lexeme embeddings and context vector - second score\n",
    "                    sim2 = distance.cosine(lexeme_embed,context_vec)\n",
    "                else:\n",
    "                    sim2 = 0\n",
    "            else:\n",
    "                    sim2 = 0\n",
    "            # score value\n",
    "            score = sim1 + sim2 \n",
    "          \n",
    "            score_list.append(score)\n",
    "            \n",
    "        if(len(wordnet.synsets(word))!=0):\n",
    "            #Get predicted synset\n",
    "            \n",
    "            max_score_index = np.argmax(score_list)\n",
    "            pred_synset = wordnet.synsets(word)[max_score_index]\n",
    "            disambiguated[word] = pred_synset\n",
    "            \n",
    "           \n",
    "            if word not in sub_dict and  word+'.' in sub_dict:              \n",
    "                word=word+'.'\n",
    "            \n",
    "            if word in sub_dict:\n",
    "               #Get true synset\n",
    "            \n",
    "                wnsn = sub_dict[word].split(',')[2]\n",
    "                lemma = sub_dict[word].split(',')[0]\n",
    "                pos = sub_dict[word].split(',')[1]\n",
    "\n",
    "                true_syn = lemma+\".\"+pos+\".0\"+wnsn\n",
    "             \n",
    "                true_synset_arr=true_syn.split(\".\")\n",
    "                predicted_synset_arr=pred_synset.name().split(\".\")\n",
    "\n",
    "            #If more than 1 true synset exists -> list\n",
    "                if ';' in true_synset_arr[2]:\n",
    "                    true_synset_arr_1=true_synset_arr[2].split(\";\")\n",
    "                    true_synset_arr_1=[i.zfill(2) for i in true_synset_arr_1]\n",
    "                else:\n",
    "                    true_synset_arr_1=true_synset_arr[2]\n",
    "            \n",
    "            #Comparison of predicted and true synsets for accuracy calculation\n",
    "                if (predicted_synset_arr[1]==true_synset_arr[1] and predicted_synset_arr[2] in true_synset_arr_1):\n",
    "                    correct+=1\n",
    "                    count+=1\n",
    "                    \n",
    "                else:\n",
    "                    count+=1\n",
    "                    \n",
    "                score_list = []\n",
    "            else:\n",
    "                score_list = []\n",
    "        else:\n",
    "            score_list = []\n",
    "            \n",
    "    #Returns the number of matching words and the total number of words considered    \n",
    "    return correct, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f341b2c",
   "metadata": {},
   "source": [
    "### Evaluation on datasets\n",
    "\n",
    "#### Evaluation steps are same as done for baseline. Only the function called differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed1e5a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using sbert embeddings on senseval2 - 35.45263157894737\n"
     ]
    }
   ],
   "source": [
    "#Senseval2 computation - sbert embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_senseval2:\n",
    "\n",
    "    correct,count = fun_lesk_sbert(data_dict_senseval2[i][0],data_dict_senseval2[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using sbert embeddings on senseval2 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7afd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using sbert embeddings on senseval3 - 34.82740855229263\n"
     ]
    }
   ],
   "source": [
    "#Senseval3 computation - sbert embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "for i in data_dict_senseval3:\n",
    "    correct,count = fun_lesk_sbert(data_dict_senseval3[i][0],data_dict_senseval3[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using sbert embeddings on senseval3 -\",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf78dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using sbert embeddings on semcor - 36.19737750172533\n"
     ]
    }
   ],
   "source": [
    "#Semcor computation - sbert embeddings\n",
    "\n",
    "correct =0\n",
    "count = 0\n",
    "correct_total = 0\n",
    "count_total = 0\n",
    "\n",
    "#Running for 5000 sentences was taking more than 24 hours, hence I am selecting only 500 sentences\n",
    "#Evaluation is done only on 500 sentences of Semcor\n",
    "new_index_list = index_list[0:500]\n",
    "for i in new_index_list:\n",
    "    correct,count = fun_lesk_sbert(data_dict_semcor[i][0],data_dict_semcor[i][1])\n",
    "    count_total += count\n",
    "    correct_total += correct\n",
    "    \n",
    "accuracy = correct_total/count_total\n",
    "print(\"Accuracy using sbert embeddings on semcor -\",accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
