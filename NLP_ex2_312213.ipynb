{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19105c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the required libraries\n",
    "\n",
    "import nltk\n",
    "from nltk import grammar, parse\n",
    "import nltk.tag\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89885d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the CONLL-Train data and extract from each sentence all chunks and the sequence of tags that make up that chunk.\n",
    "#Function to read the file and tokenize the words to form chunks and sentences\n",
    "def read_conll(filepath):\n",
    "    result = []\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "    file = open(filepath)\n",
    "    sentence = []\n",
    "    sentence1 = \"\"\n",
    "    sentence2 = []\n",
    "    for line in file:\n",
    "        line = line.strip('\\n')\n",
    "        if not line.strip(' '):\n",
    "            result.append(sentence)\n",
    "            result1.append(sentence1)\n",
    "            result2.append(sentence2)\n",
    "            sentence = []\n",
    "            \n",
    "            continue\n",
    "        (word,pos,tag) = line.split(' ')\n",
    "        sentence.append((pos,tag)) # appending only pos and tag\n",
    "        sentence2.append((word,pos,tag)) #appending word,pos and tag\n",
    "        sentence1= sentence1 + \" \" + word # taking only the words to form sentences\n",
    "    return result,result1,result2\n",
    "    \n",
    "\n",
    "conll_train, words_train, x = read_conll('train.txt')\n",
    "conll_test, words_test, tree_test = read_conll('test.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfadf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to form chunks and trees from the sentence and counting\n",
    "\n",
    "def extract_chunks(words):\n",
    "    tree = {}\n",
    "    tokenized_sent = nltk.tokenize.word_tokenize(words)\n",
    "    pos_tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "    pos_tags = [t for (w,t) in pos_tagged_sent]\n",
    "    ct = nltk.tag.CRFTagger()\n",
    "    ct.train(conll_train,'crf.model')\n",
    "    ct.evaluate(conll_test)\n",
    "    chunks = ct.tag(pos_tags)\n",
    "    counts = Counter( tag for word,  tag in chunks) #Counting the occurence of each tag in each sentence\n",
    "    print(counts)\n",
    "    chunked_sent = [(w,p,t) for (w,(p,t)) in zip(tokenized_sent,chunks)]\n",
    "    #pprint(chunked_sent)\n",
    "    tree = nltk.chunk.conlltags2tree(chunked_sent)\n",
    "    #print(tree)\n",
    "    return tree, chunked_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011dceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935d16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6778f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8936"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3d132f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/75yp7_695hg75lbf0tpmjs9w0000gn/T/ipykernel_4437/3309341858.py:8: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  ct.evaluate(conll_test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I-NP': 11, 'B-NP': 9, 'I-VP': 6, 'B-PP': 5, 'O': 3, 'B-VP': 2, 'B-ADJP': 1})\n",
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  (PP if/IN)\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  (ADJP due/JJ)\n",
      "  (PP for/IN)\n",
      "  (NP release/NN tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VBP to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n",
      "Counter({'I-NP': 22, 'B-NP': 16, 'B-PP': 9, 'I-VP': 9, 'O': 4, 'B-VP': 3, 'B-ADJP': 1})\n",
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  (PP if/IN)\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  (ADJP due/JJ)\n",
      "  (PP for/IN)\n",
      "  (NP release/NN tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VBP to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.\n",
      "  (NP Chancellor/NNP)\n",
      "  (PP of/IN)\n",
      "  (NP the/DT Exchequer/NNP Nigel/NNP Lawson/NNP)\n",
      "  (NP 's/POS restated/JJ commitment/NN)\n",
      "  (PP to/TO)\n",
      "  (NP a/DT firm/JJ monetary/JJ policy/NN)\n",
      "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "  (NP a/DT freefall/NN)\n",
      "  (PP in/IN)\n",
      "  (NP sterling/NN)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT past/JJ week/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Since the dataset is huge I am taking only 2 sentences here as it was taking too long to run\n",
    "for i in range (2):\n",
    "    tree, chunked_sent = extract_chunks(words_train[i])\n",
    "    print(tree) #Printing the tree structure and the counter\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f91eebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree = nltk.chunk.conlltags2tree(tree_test[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b182c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing grammar by hand from the above output\n",
    "grammar = r\"\"\"NP: {<[CDJNP].*>+}\n",
    " {<DT|PRP.*>?(<JJ>)*<NN.*><PP>?}\n",
    " {<PRP>}\n",
    " {<DT>?<JJ>*<NN>}\n",
    " {<DT><NN>}\n",
    " VP:{<VBZ>*<VBN>*<TO>*<VB>}\n",
    " PP: {<IN><NP>}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eb78180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/75yp7_695hg75lbf0tpmjs9w0000gn/T/ipykernel_4437/2876804960.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  cp.evaluate(test_tree)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'leaves'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cp \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpParser(grammar)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tree\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/internals.py:426\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.newFunc\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewFunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/chunk/api.py:40\u001b[0m, in \u001b[0;36mChunkParserI.evaluate\u001b[0;34m(self, gold)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse accuracy(gold) instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, gold):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/chunk/api.py:55\u001b[0m, in \u001b[0;36mChunkParserI.accuracy\u001b[0;34m(self, gold)\u001b[0m\n\u001b[1;32m     53\u001b[0m chunkscore \u001b[38;5;241m=\u001b[39m ChunkScore()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m correct \u001b[38;5;129;01min\u001b[39;00m gold:\n\u001b[0;32m---> 55\u001b[0m     chunkscore\u001b[38;5;241m.\u001b[39mscore(correct, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse(\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaves\u001b[49m()))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunkscore\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'leaves'"
     ]
    }
   ],
   "source": [
    "#Trying to evaluate the grammar on the test tree\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.evaluate(test_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df27dd5",
   "metadata": {},
   "source": [
    "Error is happening due to some structure issue of the test_tree which I was stuck in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
